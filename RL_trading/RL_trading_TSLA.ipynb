{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-trading-TSLA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsuVC8SboeTE",
        "outputId": "92938d02-3d71-483e-ff6c-f64d7229b0a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yATw6cZowiZ",
        "outputId": "ae369f53-9373-4a00-940d-83d297009598"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 tensorflow==1.15.0 stable-baselines gym-anytrading gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: stable-baselines in /usr/local/lib/python3.7/dist-packages (2.10.2)\n",
            "Requirement already satisfied: gym-anytrading in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (4.1.2.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines) (1.1.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-42W-iNo166",
        "outputId": "0c19cd1e-18a4-42b7-cb32-cf70c5935f9e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import gym\n",
        "import gym_anytrading\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import A2C\n",
        "from stable_baselines.common.callbacks import CheckpointCallback, EvalCallback"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
            "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOxo7Kjyoft7"
      },
      "source": [
        "df = pd.read_csv('gdrive/My Drive/stock/TSLA.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "UwXU0DcGpcH6",
        "outputId": "1cf5f207-2702-433e-fc03-339282190d94"
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-06-30</td>\n",
              "      <td>5.158000</td>\n",
              "      <td>6.084000</td>\n",
              "      <td>4.660000</td>\n",
              "      <td>4.766000</td>\n",
              "      <td>4.766000</td>\n",
              "      <td>85935500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-07-01</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.184000</td>\n",
              "      <td>4.054000</td>\n",
              "      <td>4.392000</td>\n",
              "      <td>4.392000</td>\n",
              "      <td>41094000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-07-02</td>\n",
              "      <td>4.600000</td>\n",
              "      <td>4.620000</td>\n",
              "      <td>3.742000</td>\n",
              "      <td>3.840000</td>\n",
              "      <td>3.840000</td>\n",
              "      <td>25699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-07-06</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.166000</td>\n",
              "      <td>3.222000</td>\n",
              "      <td>3.222000</td>\n",
              "      <td>34334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-07-07</td>\n",
              "      <td>3.280000</td>\n",
              "      <td>3.326000</td>\n",
              "      <td>2.996000</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>34608500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2574</th>\n",
              "      <td>2020-09-21</td>\n",
              "      <td>453.130005</td>\n",
              "      <td>455.679993</td>\n",
              "      <td>407.070007</td>\n",
              "      <td>449.390015</td>\n",
              "      <td>449.390015</td>\n",
              "      <td>109476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2575</th>\n",
              "      <td>2020-09-22</td>\n",
              "      <td>429.600006</td>\n",
              "      <td>437.760010</td>\n",
              "      <td>417.600006</td>\n",
              "      <td>424.230011</td>\n",
              "      <td>424.230011</td>\n",
              "      <td>79580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2576</th>\n",
              "      <td>2020-09-23</td>\n",
              "      <td>405.160004</td>\n",
              "      <td>412.149994</td>\n",
              "      <td>375.880005</td>\n",
              "      <td>380.359985</td>\n",
              "      <td>380.359985</td>\n",
              "      <td>95074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2577</th>\n",
              "      <td>2020-09-24</td>\n",
              "      <td>363.799988</td>\n",
              "      <td>399.500000</td>\n",
              "      <td>351.299988</td>\n",
              "      <td>387.790009</td>\n",
              "      <td>387.790009</td>\n",
              "      <td>96561100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2578</th>\n",
              "      <td>2020-09-25</td>\n",
              "      <td>393.470001</td>\n",
              "      <td>408.730011</td>\n",
              "      <td>391.299988</td>\n",
              "      <td>407.339996</td>\n",
              "      <td>407.339996</td>\n",
              "      <td>67068400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2579 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date        Open        High  ...       Close   Adj Close     Volume\n",
              "0     2010-06-30    5.158000    6.084000  ...    4.766000    4.766000   85935500\n",
              "1     2010-07-01    5.000000    5.184000  ...    4.392000    4.392000   41094000\n",
              "2     2010-07-02    4.600000    4.620000  ...    3.840000    3.840000   25699000\n",
              "3     2010-07-06    4.000000    4.000000  ...    3.222000    3.222000   34334500\n",
              "4     2010-07-07    3.280000    3.326000  ...    3.160000    3.160000   34608500\n",
              "...          ...         ...         ...  ...         ...         ...        ...\n",
              "2574  2020-09-21  453.130005  455.679993  ...  449.390015  449.390015  109476800\n",
              "2575  2020-09-22  429.600006  437.760010  ...  424.230011  424.230011   79580800\n",
              "2576  2020-09-23  405.160004  412.149994  ...  380.359985  380.359985   95074200\n",
              "2577  2020-09-24  363.799988  399.500000  ...  387.790009  387.790009   96561100\n",
              "2578  2020-09-25  393.470001  408.730011  ...  407.339996  407.339996   67068400\n",
              "\n",
              "[2579 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raSQPyIBpmTJ",
        "outputId": "a864def7-2768-4237-e0a2-d156ca7d9593"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date          object\n",
              "Open         float64\n",
              "High         float64\n",
              "Low          float64\n",
              "Close        float64\n",
              "Adj Close    float64\n",
              "Volume         int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4_gtg8QpqFM"
      },
      "source": [
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siIj_4hlp0ut"
      },
      "source": [
        "df.set_index('Date', inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPSRMRd1q1RP"
      },
      "source": [
        "df = df.sort_values(by=[\"Date\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "i6s7oYNxrLLR",
        "outputId": "1e83dbf3-7841-4c39-be6f-60500dc27cff"
      },
      "source": [
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-06-30</th>\n",
              "      <td>5.158000</td>\n",
              "      <td>6.084000</td>\n",
              "      <td>4.660000</td>\n",
              "      <td>4.766000</td>\n",
              "      <td>4.766000</td>\n",
              "      <td>85935500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-01</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.184000</td>\n",
              "      <td>4.054000</td>\n",
              "      <td>4.392000</td>\n",
              "      <td>4.392000</td>\n",
              "      <td>41094000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-02</th>\n",
              "      <td>4.600000</td>\n",
              "      <td>4.620000</td>\n",
              "      <td>3.742000</td>\n",
              "      <td>3.840000</td>\n",
              "      <td>3.840000</td>\n",
              "      <td>25699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-06</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.166000</td>\n",
              "      <td>3.222000</td>\n",
              "      <td>3.222000</td>\n",
              "      <td>34334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-07</th>\n",
              "      <td>3.280000</td>\n",
              "      <td>3.326000</td>\n",
              "      <td>2.996000</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>34608500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-21</th>\n",
              "      <td>453.130005</td>\n",
              "      <td>455.679993</td>\n",
              "      <td>407.070007</td>\n",
              "      <td>449.390015</td>\n",
              "      <td>449.390015</td>\n",
              "      <td>109476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-22</th>\n",
              "      <td>429.600006</td>\n",
              "      <td>437.760010</td>\n",
              "      <td>417.600006</td>\n",
              "      <td>424.230011</td>\n",
              "      <td>424.230011</td>\n",
              "      <td>79580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-23</th>\n",
              "      <td>405.160004</td>\n",
              "      <td>412.149994</td>\n",
              "      <td>375.880005</td>\n",
              "      <td>380.359985</td>\n",
              "      <td>380.359985</td>\n",
              "      <td>95074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>363.799988</td>\n",
              "      <td>399.500000</td>\n",
              "      <td>351.299988</td>\n",
              "      <td>387.790009</td>\n",
              "      <td>387.790009</td>\n",
              "      <td>96561100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>393.470001</td>\n",
              "      <td>408.730011</td>\n",
              "      <td>391.299988</td>\n",
              "      <td>407.339996</td>\n",
              "      <td>407.339996</td>\n",
              "      <td>67068400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2579 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High  ...   Adj Close     Volume\n",
              "Date                                ...                       \n",
              "2010-06-30    5.158000    6.084000  ...    4.766000   85935500\n",
              "2010-07-01    5.000000    5.184000  ...    4.392000   41094000\n",
              "2010-07-02    4.600000    4.620000  ...    3.840000   25699000\n",
              "2010-07-06    4.000000    4.000000  ...    3.222000   34334500\n",
              "2010-07-07    3.280000    3.326000  ...    3.160000   34608500\n",
              "...                ...         ...  ...         ...        ...\n",
              "2020-09-21  453.130005  455.679993  ...  449.390015  109476800\n",
              "2020-09-22  429.600006  437.760010  ...  424.230011   79580800\n",
              "2020-09-23  405.160004  412.149994  ...  380.359985   95074200\n",
              "2020-09-24  363.799988  399.500000  ...  387.790009   96561100\n",
              "2020-09-25  393.470001  408.730011  ...  407.339996   67068400\n",
              "\n",
              "[2579 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6LdXe8CseAk"
      },
      "source": [
        "## Random Action"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHP3I81nrLaa"
      },
      "source": [
        "env = gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en0MPgEnrnKX",
        "outputId": "f06d73d0-6230-4350-b2dc-7fce4e107d40"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "XSfcC5M8rq7A",
        "outputId": "b7f4c8d8-79b0-49b3-e69c-1ae534c8f5e9"
      },
      "source": [
        "state = env.reset()\n",
        "while True: \n",
        "    action = env.action_space.sample()\n",
        "    n_state, reward, done, info = env.step(action)\n",
        "    if done: \n",
        "        print(\"info\", info)\n",
        "        break\n",
        "        \n",
        "plt.figure(figsize=(15,6))\n",
        "plt.cla()\n",
        "env.render_all()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info {'total_reward': -0.2019999999999995, 'total_profit': 0.6786860438965011, 'position': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAGQCAYAAADfvZNdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXhcVf3H8c83S7M1TZt0b7aWrWxtgdKyyCYoIov8RBCNIiBUxIVNECmKqBVRUEAULCAIBBBwARQQrLIKhZaWltKylWZpaZtma5s9mfP7496USTKTTCbLTDvv1/P0SefeO3dOJjPJfO4553vMOScAAAAAQPxIinUDAAAAAABdEdQAAAAAIM4Q1AAAAAAgzhDUAAAAACDOENQAAAAAIM4Q1AAAAAAgzhDUACQEM3Nmtnus2xEtMzvazCpj3Q4MnJn92MzuH6bH+pmZbTGzjWZWaGbbzSx5OB4bADAwBDUAMeV/cOz8FzCzpqDbJWHuM6ihxcyeM7Nm/zG3mNlfzWzSYJ0/HpjnejOr9v9db2YW5tgTzewlM6vzP+DfaWbZQfvTzOyPZrbV339p0L5DzOxZM6sxsyozeyT4ueyrHWY2y8yWmlmj/3VWNN9DlM/RVUGvvWYz6wi6vaqX+z1nZucNUhuO9t8H281sm5m9Y2bnRHmuQkmXSdrHOTfROVfunBvpnOuItt29/XzCHH+mma02swYz+8DMjvC3l3R77zf6F1MO8venmdntZrbJfy09YWZTIjm3v+8Mf982M3vbzE7tdt9pZvYPf/8WM/tlf54HABgOBDUAMeV/cBzpnBspqVzSyUHbSoexKd/227C7pJGSbhjGx+7CzFKG4LTzJJ0qaaakGZJOlvSNMMfmSPqZpMmS9pY0RdKvgvb/WNIekookHSPpCjP7jL9vjKSFkor9/dsk3R1JO8xshKTHJN3vn+dPkh7zt/f3e+g359zPg16LF0h6Jei1uO9gPU4ENvhtGCXp+5LuMLN9uh8UweukUFK1c27zYDQqgp9P9+M/Jel6SedIypZ0pKS1kuScK+323r/Q3/eGf/eLJB0q7+c8WVKtpN9Gcm4/0N0v6VJ5z+Hlkh4ws/FB38ezkv4jaaKkfP94AIgrBDUAccm/on6TmW3w/93kb8uS9JSkyUFX4yeb2Rwze8XvBfrIzG4N9wGyN865Okl/lxTckzM9qJfoHTM7w98+1X+8JP/2HWa2Oeh+95nZxf7/zwm6wr/WzL4RdNzRZlZpZt83s42S7jazDDO7x8xqzextSQdH90zu8DVJNzrnKp1z6yXdKOnsMM/BA865p51zjc65Wkl3SDq827l+6pyrdc6t9vef7d/3KefcI865rc65Rkm3hrhvuHYcLSlF0k3OuRbn3C2STNIn+/s9DDYzO8zMXjezev/rYf72BZKOkHSr/1q81d9+s5lVmNfruDS4tydSzvN3eSFlHzM728xeNrPfmFm1pB+bWY6Z3Wte72WZmV1tZklmdpy8MNL5PrnHzIr9XquUcO3uw9Hq/efT3bWSfuKce9U5F3DOrfd/bqF8TdK9zjnn354q6V/OuU3OuWZJf5YUHJZ7O3e+pDr/teicc/+U1CBpN3//2fLC8K+dcw3OuWbn3IoIvn8AGFYENQDxar6kQ+QFppmS5ki62jnXIOkE+b0O/r8NkjokXSJprLwr8cfKu0rfL2aWJ+nzkt73b2fJ+8D7gKTxks6U9Hsz28c596GkrZIO8O9+pKTtZra3f/soSc/7/98s6SR5V/jPkfQbMzsw6KEnSsqV1ws1T9I18j5Y7ibpeHkfZIPb+Xsz+30/vrV9Jb0ZdPtNdf3g25sjJa3yH3eMpEn9ONeO+0bQjn0lrQj6sC5JK7rtj/Z76MIs8iGTZpYr6Z+SbpGUJ+nXkv5pZnnOufmSXpTfI+uc+7Z/t9flvXZz5b12HjGz9H62McnM/k/SaEkr/c1z5fUcTZC0QF4vU46kafJeb2dJOsc59291fZ+cHXzucO32hwNeGaZJff18gtueLGm2pHFm9r5/IeJWM8sIcWyRvNfJvUGb75J0uH8RJlNSibwLNJGce4mk1WZ2ipkl+8MeW/y2St7vlXVm9pQ/7PE5M9s/zPcMADFDUAMQr0rkXTHf7JyrkncF/avhDnbOLfWvrrc759ZJ+oO8D66RusXM6iVtkRf2vuNvP0nSOufc3f65l0n6i6TT/f3PSzrKzCb6tx/1b0+VF8re9Nv3T+fcB/4V/uclPSOvR6NTQNI1fk9Fk6QzJC1wztU45yrkhYTg7/dC51x/guhISfVBt+sljewrsPhDzL4m6UdB5+m8f/C5stWNmc3w73d5hO3ovq/7uSP+HvyQc7nfi7nBD7azzSzPzC6RF7gjdaKk95xz9/mvgQclrZE39DIk59z9zrlq//gbJaVJ2ivCx5tsZnXyXovXSPqqc+4df98G59xvnXPtklr97+MHzrlt/uv+RvXyPumLc+4k59wvwuzu6+cTbIKkVElfkPc6nyXvgsbVIY49S9KL/oWPTu9JqpC0Xt7FkL0l/SSSc/tz8O6VF5Bb/K/f8C/ySF6P25ny3lOT5YXwsEM4ASBWCGoA4tVkSWVBt8v8bSGZ2Z5+b8BGM9sq6efyAlekvuucy5E3J2aMvA9zktfDNdcf4ljnf4AukdcDJnlB7Wh5PQIvSHpOXkA8St6Hz4DfvhPM7FV/+GSdpM92a1+VP8Qr+Puv6Pb9R8S6FsW43d+8XV5w7DRK0vZuvSPdz3OIvA+5X3DOvRt0ns77B59rW7f77i6vB+Qi59yLQbt6a0f3fd3P3Z/voVDez+4weT2yH0q6T9JyecPqHg9xn3C6vxbl354S4lhJkpl9zw+J9f7PO0eRvx43OOdGO+dynXOznHMPBe0Lfk2MlRdYur9PwrZrgPr6+QRr8r/+1jn3kXNui7yeyM+GOPYsefPdgv1OXrjNk5Ql6a/ye9T6Orc/7POX8t6XI+S9F++0jwufNEl6yR8a2SpvPmqevDAIAHGDoAYgXm2Q90G7U6G/TZJCfTC/TV4vxx7OuVGSrpI3f6ZfnHMr5RXS+J3fU1Mh6Xn/g3Pnv5HOuW/6d3le3lX9o/3/vyRvTtaOYY9mliavF+4GSROcc6MlPdmtfd2/p48kFQTdLuzH97CjKIZz7gJ/8yp5gaXTTHUdktiFmR0gL8yc65xbFHTuWr9tYc/lD2X7t7x5bPd1O3Vv7VglaUa3HrIZ3fZH+j2UO+e+7bx5dFXOuV855/Z2zhU4574b1LsSie6vRcn7eXTOierys/Pno10hr1d0jP/zrlcUr8cQgh9ri6Q29XyfhJsH1tu5ItHXz+fjE3uvk8puj9Hj8czscHlB+NFuu2ZJusfvUW6RN8RzjpmNjeDcsyS94Jxb4s9fe13SYknH+ftXhGoLAMQbghqAePWgpKvNbJyZjZU3hK6zMtsmSXlmlhN0fLa8IVLbzWy6pG8qen+SN7zqFEn/kLSnmX3VzFL9fwd3zkNzzr0n7wr9V+QFuq1++07Tx/PTRsjrHaiS1G5mJ0j6dB9teFjSD8xsjJnl6+OhmNG6V9KlZjbFzCbLK9t+T6gDzWw/SU9L+o5z7okw57rab9t0Sed3nsu8inv/kXSrc+72MPcN147n5M01/K55hWM653v9p7/fQ2dP5iB5Ut5r4Mt+IY4vStpH3mtD8n7e04KOz5bULu/nnWJmP1LPnqgB84f4PSxpgZll+wH5UkVewbB7u/vynHr/+XR3t6TvmNl4f27jJfr4Oev0NUl/cc5175V7XdJZ5hVLSZU333SD33vW17lfl3REZw+af9HhCH08R+1+SYeY2XH+fLeL5YXe1RE/EwAwDAhqAOLVz+QVBVghr5DCG/42OefWyAtya/3hiJMlfU/Sl+UNw7pDXpW4qPjDoW6W9EP/A+Sn5c1p2SBpo7yy4GlBd3leXhn0iqDb5rdZ/jm+K+9Dda3fzr6G3l0rbxjbh/Lms3XpmTJvjalQQSicP0h6Qt5z+Za8eTl/CDrfdvu4MuFlksZJustCryN2jaQP/PY9L+lXzrmn/X3nyfvw/+Og+24Pum/YdvjP+6nyhsLVSTpX0qn+9j6/h6HinKuWN1fxMknV8nrLTgoKDTdL+oJ5FTpvkfQveUH3XXnPUbO6DlkcTN+RV9Fwrbze3Ack/THC+3Zvt/wCG1eFOrivn48/5PapoLv8VF5oeldeCFomrwCK/OPT5fU6dh/2KHnv52Z5c9Wq5A1r/L9Izu3PAf2xpEfNbJu83uyfO+ee8fe/I+/Cyu3y3o+fk3RK0OsMAOKC9TI9AQAAAAAQA/SoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJwhqAEAAABAnCGoAQAAAECcIagBAAAAQJxJidUDjx071hUXF8fq4QEAAAAgppYuXbrFOTcu1L6YBbXi4mItWbIkVg8PAAAAADFlZmXh9jH0EQAAAADiDEENAAAAAOIMQQ0AAAAA4gxBDQAAAADiDEENAAAAAOIMQQ0AAAAA4gxBDQAAAADiDEENAAAAAOIMQQ0AAAAA4gxBDQAAAEDslJZKxcVSUpL3tbQ01i2KCymxbgAAAACABFVaKs2bJzU2erfLyrzbklRSErt2xQF61AAAAADExvz5H4e0To2N3vYER1ADAAAAEBvl5f3bnkAIagAAAABio7Cwf9sTCEENAAAAQGwsWKDWtPSu2zIzpQULYtOeOEJQAwAAABATLV88Uz8+6SJV502Uk2lDznh13P6HhC8kIhHUAAAAAMTI029t1AO7H6G3X1mhZ97aoMMu+KNennt8rJsVFwhqAAAAAGKidHG5ivIydfhuY3XUnuOUnZaiJ97cEOtmxQWCGgAAAIBh996mbXrtwxp9aU6hkpJM6anJOn6/iXp61Ua1tHfEunkxR1ADAAAAMOweeK1cqcmm0w/K37Ht5JmTta25Xc+/UxXDlsUHghoAAACAYdXU2qG/LK3UCftNUt7ItB3bD9stT7lZI/TEio9i2Lr4QFADAAAAMKz+sWKDtja368tzu66XlpqcpBP2m6h/v71Jja3tMWpdfCCoAQAAABhWpYvLtdu4LM2dmttj38kzJ6uprUOLVm+OQcviB0ENAAAAwLBZtaFeyyvqVDK3SGbWY//BxbmaMCot4as/EtQAAAAADJsHFpcrLSVJpx2YH3J/cpLpxP0n67l3qrS1uW2YWxc/IgpqZjbazB41szVmttrMDu2238zsFjN738xWmNmBQ9NcAAAAADur7S3t+vuy9TppxmTlZKaGPe7kmZPU2hHQM6s2DWPr4kukPWo3S3raOTdd0kxJq7vtP0HSHv6/eZJuG7QWAgAAANglPL58gxpaO1RySGGvx80qGK2C3IyEHv7YZ1AzsxxJR0q6S5Kcc63Oubpuh31O0r3O86qk0WY2adBbCwAAAGCn5JxT6eIy7T1plA4oGN3rsWamk2dM1kvvb1FNQ2v0D1paKhUXS0lJ3tfS0ujPNcwi6VGbKqlK0t1mtszM7jSzrG7HTJFUEXS70t/WhZnNM7MlZrakqopF7AAAAIBdXenKUhXfVKzknyTrX9WnqbjgjZBFRLo7eeZkdQScnnoryjXVSkulefOksjLJOe/rvHk7TViLJKilSDpQ0m3OuQMkNUi6MpoHc84tdM7Nds7NHjduXDSnAAAAALCTKF1ZqnlPzFNZfZmcnDqSqvSn1VepdGXfYWn6xGztPn5k9MMf58+XGhu7bmts9LbvBCIJapWSKp1zi/3bj8oLbsHWSyoIup3vbwMAAACQoOYvmq/Gtq5hqam9UfMX9R2WOoc/Lv6wRhvrm/v/4OXl/dseZ/oMas65jZIqzGwvf9Oxkt7udtjjks7yqz8eIqneORdlHyUAAACAXUF5fehQFG57dyfNnCTnpH+u7H+06MgvCL2jsPdCJvEi0qqP35FUamYrJM2S9HMzu8DMLvD3PylpraT3Jd0h6cJBbykAAACAnUphTuhQFG57d7uNG6nReYv1nf8eqqRrk1R8U3FEwybrGlt1w1FfU1NqWtcdmZnSggURPXasRRTUnHPL/bllM5xzpzrnap1ztzvnbvf3O+fct5xzuznn9nfOLRnaZgMAAACIdwuOXaDM1Mwu2zJTM7Xg2MjCUunKUq1u/pWaApvk5FRWX6Z5T8zrNazVN7Xpq3e9pruKDlXZdTdJRUWSmfd14UKppGRA39NwMedcTB549uzZbskS8hwAAACwK7t72X2a99hlarctKsop1IJjF6hk/8jCUvFNxSqrL+uxvSinSOsuXtdj+/aWdn31rsV6a329Fn51to6ZPn6gzR9SZrbUOTc71L6U4W4MAAAAgMSx7+gTNaU5V3eeNVvH7TOhX/ftbY5bIOD04KoHNH/RfJXXlyt/VIEm6RxtqZqj35ccGPchrS+RzlEDAAAAgH5bXlErSZpV2Psi16GEm8uWFBir/X/5Q5372Pk7Sv9XbC3X6/W/0P8dvlbH7ztxQG2OBwQ1AAAAAENmeUWdCnIzNHZkWt8HdxNujtt3Z/9Ia1vvVGtHU5d9zlr06Hs3DKi98YKgBgAAAKB3paVScbGUlOR9Le278mKnZeV1mlUwJqqHLdm/RAtPXqiinCKZTEU5RVp48kL9+uRvq8VtDnmfSEv/xzvmqAEAAAAIr7RUmjdPavQXri4r825LfVZQ3LS1WR/VN2tWQf+HPXYq2b8kZPGRwpzCkIVGIi39H+/oUQMAAAAQ3vz5H4e0To2N3vY+LCuvkyQdEMX8tL4MtPR/vCOoAQAAAAivPMxQwnDbgyyrqFVqsmmfSaMGuVHhh0VGWvo/3jH0EQAAAEB4hYXecMdQ2/uwvLxO+0wapfTU5CFoWPhhkbsCetQAAAAAhLdggZTZdYihMjO97b3oCDitXF+vAwqjKySS6AhqAAAAAMIrKVHH7X9Q5ahxCsi0cfR4uYUL+ywk8u6mbWps7RhQIZFExtBHAAAAAL2qOfV0fWLVGE2fmK01G7fp2eOO1B593KezkAhBLTr0qAEAAADoVXVDiyTpCwflS5IWrQm9hlmw5RW1GpOZqqK8zD6PRU8ENQAAAAC9qtneKknad3KO9p40Sv+JKKjVaVbBaJnZUDdvl0RQAwAAANCr6gYvqOWNHKFjp4/X0rJa1Te2hT1+W3Ob3tu8XbMKKCQSLYIaAAAAgF7V+EEtN2uEjpk+Xh0Bp+ffqwp7/IrKejknzRqCha4TBUENAAAAQK+qG1plJo3JHKFZBaOVmzVC/+1l+OPyCr+QSD5BLVoENQAAAAC9qmlo0eiMVCUnmZKTTEfvOU7PvbNZHQEX8vhl5XWaNi5LOZmpw9zSXQdBDQAAAECvahpalZs1YsftY6aPV21jm5ZX1PY41jmn5RW1lOUfIIIaAAAAgF5Vb29VXlbajttH7jlOyUmmRat7Dn+srG3Slu2tOqCQQiIDQVADAAAA0KvuPWo5GamaXTQmZJn+zvlpB9CjNiAENQAAAAC9qmloVe7IEV22Hbv3eK3ZuE3r65q6bF9WXqe0lCTtNTF7OJu4yyGoAQAAAAgrEHCqbWxVXlbXoPbJ6eMlqUf1x+UVtZqRn6PUZKLGQPDsAQAAAAirrqlNAacuQx8labdxI1WQm9ElqLW2B/TWhq0UEhkEBDUAAAAAYdU0tEjqGdTMTMdOn6CXP9ii5rYOSdKajVvV2h7QrAIKiQwUQQ0AAABAWNXbWyWpS9XHTsdMH6/mtoBe+aBakjc/TZJmFdKjNlAENQAAAABh1TR4Qa17j5okzZ2aq4zUZC1as0mSV/FxfHaaJuekD2sbd0UENQAAAABhVftBLW9kz6CWnpqsT+wxVv9dU+UvdF2nWQWjZWbD3cxdDkENAAAAQFidPWpjMnsGNcmr/ri+rkmLP6zRh1saGPY4SAhqAAAAAMKqaWhVdnqKRqSEjg7H7OWV6f/1s+9Kkg6gkMigIKgBAAAACKu6oecaasEm5qTrwg2L9esrPqe115+suZ88UCotHcYW7ppSYt0AAAAAAPGrpqElZCGRHUpLdcnDv1JqS7N3u7xcmjfP+39JydA3cBdFjxoAAACAsKq3tyo3RGn+HebP/zikdWpslObPH9qG7eIIagAAAADCqulj6KPKy/u3HREhqAEAAAAIyTmnmoZW5YYozb9DYWH/tiMiBDUAAAAAIW1tald7wPXeo7ZggZSZ2XVbZqa3HVEjqAEAAAAIqbqhRVLoxa53KCmRFi6UiookM+/rwoUUEhkgqj4CAAAACKlzsetei4lIXigjmA0qetQAAAAAhFTtB7Vehz5iSBDUAAAAAIT0cY8aQW24RTT00czWSdomqUNSu3Nudrf9R0t6TNKH/qa/Oud+MnjNBAAAADDcCGqx0585asc457b0sv9F59xJA20QAAAAgPhQvb1VWSOSlZ6aHOumJByGPgIAAAAIqaahpfc11DBkIg1qTtIzZrbUzOaFOeZQM3vTzJ4ys30HqX0AAAAAYqS6obXvio8YEpEOffyEc269mY2X9KyZrXHOvRC0/w1JRc657Wb2WUl/l7RH95P4IW+eJBWyUjkAAAAQ12oaWjVhVHqsm5GQIupRc86t979ulvQ3SXO67d/qnNvu//9JSalmNjbEeRY652Y752aPGzduwI0HAAAAMHRqGlopJBIjfQY1M8sys+zO/0v6tKS3uh0z0czM//8c/7zVg99cAAAAAMPBOafqhlbWUIuRSIY+TpD0Nz+HpUh6wDn3tJldIEnOudslfUHSN82sXVKTpDOdc26I2gwAAABgiDW0dqi1PUCPWoz0GdScc2slzQyx/fag/98q6dbBbRoAAACAWKnZzhpqsUR5fgAAAAA9VDe0SJLyKM8fEwQ1AAAAAD3UNHT2qFGePxYIagAAAAB6qPaDGsVEYoOgBgAAAKCHj3vUCGqxQFADAAAA0ENNQ6vSUpKUOSI51k1JSAQ1AAAAAD1Ub/fWUPOX6cIwI6gBAAAA6KGmoUW5VHyMGYIaAAAAgB6qG1qp+BhDBDUAAAAAPXQOfURsENQAAAAA9FDT0ErFxxgiqAEAAADooqm1Q01tHQS1GCKoAQAAAOiiuqFFkjSWYiIxQ1ADAAAA0MXHi11TTCRWCGoAAAAAuqjeEdToUYsVghoAAACALmq2e0GNqo+xQ1ADAAAA0MWOoY/MUYsZghoAAACALqobWpWabMpOS4l1UxIWQQ0AAABAFzUNLcrNGiEzi3VTEhZBDQAAAEAX3mLXVHyMJYIaAAAAgC6qG1opJBJjBDUAAAAAXXg9agS1WCKoAQAAAOiiZjtBLdYIagAAAAB2aGnv0LaWdoY+xhhBDQAAAMAOtQ1tklhDLdYIagAAAAB2qG5okSR61GKMoAYAAABgh5qGVkmiPH+MEdQAAAAA7PBxUKNHLZYIagAAAAB2qN7uBTWGPsYWQQ0AAADADjUNrUpOMuVkpMa6KQmNoAYAAABgh+qGFo3JTFVSksW6KQmNoAYAAABgh2oWu44LBDUAAAAAO9Q0ENTiAUENAAAAwA41Da3KozR/zBHUAAAAAOxQTY9aXCCoAQAAAJAktXUEVN/URlCLAwQ1AAAAAJKk2kZvDbWxIwlqsUZQAwAAACDJm58mSbnMUYs5ghoAAAAASVLN9s6gRo9arBHUAAAAAEjyColIUh5DH2OOoAYAAABAUvDQR4JarBHUAAAAAEjyetTMpDGZBLVYiyiomdk6M1tpZsvNbEmI/WZmt5jZ+2a2wswOHPymAgAAABhKNQ0tGp2RquQki3VTEl5/etSOcc7Ncs7NDrHvBEl7+P/mSbptMBoHAAAAYHCUrixV8U3FSro2ScU3Fat0ZWmPY2pY7DpuDNbQx89Jutd5XpU02swmDdK5AQAAgIQXSdDq7b7znpinsvoyOTmV1Zdp3hPzepyjenur8ijNHxciDWpO0jNmttTM5oXYP0VSRdDtSn9bF2Y2z8yWmNmSqqqq/rcWAAAASECRBq1w5i+ar8a2xi7bGtsaNX/R/C7b6FGLH5EGtU845w6UN8TxW2Z2ZDQP5pxb6Jyb7ZybPW7cuGhOAQAAACScSINWOOX15RFtr2loVS6l+eNCREHNObfe/7pZ0t8kzel2yHpJBUG38/1tAAAAAAYo0qAVinNOmckTQu6bPDJ/x/8DAafaxlbl0aMWF/oMamaWZWbZnf+X9GlJb3U77HFJZ/nVHw+RVO+c+2jQWwsAAAAkoMKcwn5tD3b/4nKlN5VoRFJGl+2mNKU2fFkrK+slSXVNbQo41lCLF5H0qE2Q9JKZvSnpNUn/dM49bWYXmNkF/jFPSlor6X1Jd0i6cEhaCwAAACSgBccuUGZqZpdt5tJ0xp6X93q/dzZu08/+8bZO3O0M3XXKHSrKKZLJVJRTpBuP+70mj/i0vrjwFa2+8XZlT99da68/WV88/QipNPJCJRga5pyLyQPPnj3bLVnSY0k2AAAAACGUrizVuX+9VK2qUsGoAo13Z2t77SF65BuHaf/8nB7HN7d16HO3vqzqhhY9ddGRGpfds5rjpq3NuvvCBfrun3+lzPaWj3dkZkoLF0olJUP5LSU8M1saZvmzQSvPDwAAAGAInb73lzS5+Y/69WFrVH5JmZ46/yrlZaXpvHtf18b65h7H/+KpNXpn0zbdcPrMkCFNkiaMStcVL9zbNaRJUmOjND+yQiUYGgQ1AAAAYCewsb5ZzklTxnhzzcZlp+mus2eroaVDX//T62psbd9x7KLVm3TP/9bp3MOn6ui9xvd63qTKitA7yvsuVIKhQ1ADAAAAdgKVtV55/vwxHxcFmT5xlH77pQO0+qOtOumO61R0U5GSrk3SCQ/PUE7uYn3/hMSxJHEAACAASURBVL36PnFhmIIk4bZjWBDUAAAAgJ1AZW2TJKlgTNeiIsdMH6/jDnpPz1UtUHl9uZyc2rRZ77bdoEdXP9T3iRcs8OakBcvM9LYjZghqAAAAwE6gsrZRSSZNzEnvse9flb+Rs67zzJrbmyJbELukxCscUlQkmXlfKSQScymxbgAAAACAvlXWNmlSToZSk3v2tVTUh55nFsmC2JK8UEYwiyv0qAEAAAA7gcq6Jk0ZnRFy30AWxEZ8IqgBAAAAO4H1tU1dCokEC7UgdmZqphYcyzyznRVBDQAAAIhzbR0BfVQfPqiV7F+ihScvVFFOkUymopwiLTx5oUr2Zzjjzoo5agAAAECc21jfrICT8rtVfAxWsn8JwWwXQo8aAAAAEOcqQqyhhl0bQQ0AAACIc+v9NdSmENQSBkENAAAAiHOVtU0ykyblENQSBUENAAAAiHOVtU2aOCpdI1L4+J4o+EkDAAAAca6ytpH5aQmGoAYAAADEucrapl4rPmLXQ1AL8soH1fr1M+/EuhkAAADADu0dAW3c2kyPWoIhqAVZVlGrW/7zvuoaW2PdFAAAAECStHFrszoCTlNGE9QSCUEtyMz80ZKkFZX1MW4JAAAA4Kn0S/Mz9DGxENSC7DclR5K0orIuxi0BAAAAPB8HNXrUEglBLUhORqqmjc3S8gp61AAAABAfKmsbvTXURqfHuikYRgS1bmYWjKZHDQAAAHFjfW2TJmSnKy0lOdZNwTAiqHUzIz9Hm7e1aGN9c6ybAgAAAKiytklTGPaYcAhq3czwC4q8Sa8aAAAA4kBlHYtdJyKCWjf7Th6llCRj+CMAAABirr0joI/qWEMtERHUuklPTdZeE7P1JgVFAAAAEGObtrWoPeAozZ+ACGohzMj3Coo452LdFAAAACSw9ZTmT1gEtRBm5udoa3O71lU3xropAAAASGCVtd7n0SmjCWqJhqAWQmdBEeapAQAAIJY6F7ueTFBLOAS1EPacMFLpqUlaXkFQAwAAQOxU1jZqfHaa0lNZQy3RENRCSElO0n6Tc7SikoIiAAAAiJ3K2ibmpyUogloYM/JHa9WGerV3BGLdFAAAACSo9XVNVHxMUAS1MGYW5Ki5LaB3N22PdVMAAACQgDoCThvqmjSFHrWERFALg4IiAAAAiKXN25rV1uEY+pigCGphFOdlalR6it4kqAEAACAGKnesocbQx0REUAvDzDSzYLTerKCgCAAAAIZf5xpq9KglJoJaL2bk5+idTdvU3NYR66YAAAAgwaz3e9RY7DoxEdR6MSN/tDoCTqs2bI11UwAAAJBgKmubNI411BIWQa0XM/2CIm+y8DUAAACGWWVtE71pCYyg1ouJOemaMCqNyo8AAAAYdpW1jcxPS2ARBzUzSzazZWb2jxD7zjazKjNb7v87b3CbGTsz8kdrRSUFRQAAADB8AgGnDXXNVHxMYP3pUbtI0upe9v/ZOTfL/3fnANsVN2bm52jtlgbVN7XFuikAAABIEFXbW9TaEaBHLYFFFNTMLF/SiZJ2mQAWqc6Fr1fSqwYAAIBhQml+RNqjdpOkKyQFejnmNDNbYWaPmlnBwJsWH2bk50gSC18DAABg2Hy82DVBLVH1GdTM7CRJm51zS3s57AlJxc65GZKelfSnMOeaZ2ZLzGxJVVVVVA0ebqMzR6g4L5OCIgAAABg2lTvWUGOOWqKKpEftcEmnmNk6SQ9J+qSZ3R98gHOu2jnX4t+8U9JBoU7knFvonJvtnJs9bty4ATR7eFFQBAAAAMOpsrZJY0eOUMYI1lBLVH0GNefcD5xz+c65YklnSvqPc+4rwceY2aSgm6eo96IjO50Z+Tn6qL5Zm7c2x7opAAAASACVtY2aQsXHhBb1Ompm9hMzO8W/+V0zW2Vmb0r6rqSzB6Nx8WJmgb/wNb1qAAAAGAbra5uYn5bg+hXUnHPPOedO8v//I+fc4/7/f+Cc29c5N9M5d4xzbs1QNDZW9p08SslJxjw1AAAADLlAwKmyrkn5owlqiSzqHrVEkjkiRXuMH0mPGgAAAIbclu0tam1nDbVER1CL0Mz80VpRWSfnXKybAgAAgF1YZV1naX7mqCUyglqEZhTkqK6xTeU1jbFuChBaaalUXCwlJXlfS0tj3SIAkkpXlqr4pmIlXZuk4puKVbqS92ZveL4A1lCDh6AWoZn5FBQZTPwhHmSlpdK8eVJZmeSc93XePMIaEGOlK0s174l5Kqsvk5NTWX2Z5j0xj995YfB8AZ7KWq9jYApBLaER1CK018RsjUhJ0ooKCooMFH+Ih8D8+VJjt97exkZvO4CYmb9ovhrbur43G9saNX8R781QeL4AT2Vtk/KyRihzREqsm4IYIqhFKDU5SdMnZmv1xq2xbspOjz/EQ6C8vH/bAQyL8vrQ78Fw2xMdzxd2GkM83aCytoneNBDU+qM4L0tl1cxRGyj+EA+BwsL+bQcwLPJHFYTcXpjDezOUcM8LzxfiyhBPNyhdWaqHK0/RE9VHMj0kwRHU+qF4bJY21DWppb0j1k3ZqfGHeAgsWKDWtPQumwIZmdKCBTFqEABJOj7/EplL67ItMzVTC47lvRnK2fteyfOF+DeE0w06p4e0uE0S00MSHkGtH4rzMhVwH1fiQXQWHLtAGSldy83yh3hgWr54pq458SLVjJ0kZ6b1o8brga9fLZWUxLppQMJq7wjo7Q9m6OCcK/0LUaaMpAlaePJClezPe7O77S3t+vcbe2qvEZepYJT3fCUHxmnB0b/l+UJccUM43YDpIQhGUOuHorwsSVJZdUOMW7JzK9m/RN+e9UslB8ap8w/xzcffxh/iAVi0erMe3OMIrXz5TVkgoHsfek5XZ83QSqqUAjHz5Fsbtb6uSQuO/6bKLi7TwmPe1/iGuzR3wudi3bS4dN2Tq7W+rkn3ffkylV9Spvcv3Kr8lrs1MeW4WDcN6GLbuEmhdwzCdAOmhyAYQa0fivO8XqB1W5inNlDtDYdr36T79fyXNim/5W7tlvWZWDdpp/bIkgpNHJWuT+w+VpL0rWN2V17WCP30n2+zSDsQA845LXzhA00bl6VPTh8vSfrcAZOVnGT6yxuVMW5d/HnpvS0qXVyurx8+VbOLcyVJ08ZmaezINL26tjrGrQM+tr6uSdceVqKWEV2nGzSmpOn9i68a8PlHp4UOgUwPSUwEtX7IzRqh7LQUetQGqL0joP+u2axPTh+vg4tzlZJkem1dTaybtdPavLVZz79bpc8fOEXJSSZJGpWeqks+tade+7BG/1q1KcYtBBLPq2tr9Nb6rTrvE9OU5L8vx2en66g9x+mvb1SqI8AFlE7bmtv0/b+s0LSxWfre8Xvt2G5mmjstV4s/rOGCE+LGz/7xtv653zHa/tvfS0VFkpk6Cgr169O/p8/XFemdjduiPvdrH9YoeduXlGJdQyDTQxIXQa0fzExFYzO1jsqPA7K0rFa1jW361D4TlDEiWfvn5+i1Dwlq0frrsvUKOOkLB+V32X7mwQXaY/xIXffUarW2B2LUOiAx3fniWuVljdDnD5zSZfsXDsrXpq0tevn9LTFqWfz5+ZNr9FF9k244Y6bSU5O77DtkWp4+qm9WeQ1/dxF7L75Xpafe2qhvH7O78uadI61bJwUCSi4v09m/u0oZI5J11h8X71isuj+qt7foOw++of3GnKQ/nLRQRTlFMpmKcoqY15rACGr9VJSXRY/aAP179SaNSE7SkXuOkyTNmZqrFZV1amqlmmZ/Oef0yJIKzS4ao2njRnbZl5KcpPkn7q2y6kbd+8q6mLQPSETvb96mRWs266xDi3sEj2P3Hq+cjFQ9ujTBhz/6a1C5pCR965xj9Zv2t3Vg4Zgehx0y1RsGuXgtF/MQW63tAV3z+CoV52Xq/COn9difPyZTfzp3jhpbO3TPt36ujsKiiNdYCwScLn34TdU2tunWLx+gcw/8qtZdvE6BawJad/E6QloCI6j1U3Fepiprm9TWQQ9FNJxzevbtTTpktzyNTEuRJM2dmqu2DqdlFbUxbt3OZ3lFnT6oaujRm9bp6L3G66g9x+mWRe+ptqF1mFsHJKY7X/xQaSlJ+sohPeeUpKUk63OzJutfqzZqa3NbDFoXB4LWoDLnlL+1Sqf8/schP8zuPn6k8rJGME8N/Va6slTFNxUr6dqkQVmL7K6XPtTaqgZdc8q+SktJDnnM9Imj9Lecdbr0kRuUXFEe8Rprt7/wgZ5/t0o/Omkf7Ts5Z0DtxK6FoNZPRXlZag84baijRH80PqjarnXVjfrUPhN2bDuoKFdmGrbhj4P9yzuWHllaqfTUJJ04I0wFKknzT9xb21vadfOi94axZUBiqtrWor++sV5fOChfeSPTQh5z2oH5amkP6J8rPhrm1sWJEGtQWZg1qILnqQGR6lyLrKy+TG4Q1iLbUNekWxa9p0/tM0HH7DW+12N3v/k6Zba3dN3Yyxprr6+r0Y3PvKuTZkxSyVwKhqArglo/Ffsl+pmnFp1n394sSTpu749/0eVkpGrviaOGJagN9i/vWGpu69ATb27QZ/ebpOz01LDH7TkhW1+aU6jfv3aPptxYuEsEVCBe3ffKOrUFAvr6J6aGPWZGfo72GD8ycYc/9nMNqrlT87S+rkkVzFNDhAZ7LbIF/1ytgHP60Un79H1wmNexKy9XY2u7dyNo6G/+AXvr7A9f1nWf319mFlX7sOsiqPVTZ4l+5qlF59m3N2q/KaM0KSejy/Y5U3P1RnntkBe92JUWkvzXqo3a1twedthjsIL8N7Ql5bfasL1ipw+oQLxqau3Qva+W6bi9J/SYMxrMzPSFg/K1tKxWa6u2D2ML40S4tabCbD9kWp4kMfwRERuUtciCwtQPLvyMbg68rYLczL7vF+Z1vD57rOb+fJH+ctn1Cpx//o6hv5PqNmv+Y79R9l8ejrxtSBgEtX4al52mjNRk1lKLQtW2Fi2rqNOn9p7YY9+cqblqbgvorQ1Du0DzrrSQ5KNLK5U/JmPHh5je/OLla+Ss61CMnTWgAvHq0aUVqmts07wQhQa6+78DpijJpL++sX4YWhZftv3oWjWldBsWmpkpLQhdfnyP8SM1JjOV4Y+IWLg1xyZkTQm5vYcQ8yiPv/lHfRYFkeS9jjO7BjqXmamWa3+mT04fr0PuvFFJTV2nzyQ1NYUdGonERlDrJzNTUV4mPWpR+O+azXJOOm6fnuO7D/YXOB3K4Y+BgNOYMAtJFuQUDNnjDoUNdU166f0tOu3A/B1rNPVmVwqoQLwpXVmqopuKdNYz07Q56+t6d+tTfd5n/Kh0HbnnOP0lAddUu7voMH3/M99WW36BZOatRbVwoVQSurJdUpJp7tS8YelR25XmMCeyL+51hcx1vRiQpDQlbfuSXnyvqu8T9GMeZQ8lJd7r2V9jTUVFsoULtdvF83TzmQdo8rYwS3OEGxKMhEZQi0JxXpbWEdT67Zm3N2nK6AztM2lUj33jstM0bVzWkAW1moZWff1Pr8tCLCRpLk17pJ2v5radZ3mAv75RKRdi7bRwwl1dDLcdGBL+UKJwJat3xg/JnfNevYseTk2BTZr3j8iGFX/hoHx9VN+sVz4YQADp4zmNN81tHbr3lXXa9vkzlFpRLgUC3lpUYUJap7nTclVZ26T1Q1jIa1eaw5zIyqsb9eTi3TRr5BUqHFW4Yy2yW0+4XfuPOUnn3vN6r4V8nHNy/ZxH2UNJyY411rq/vq2fQ3+R2AhqUSgem6WKmqaEuwo6EE2tHXrp/Sodt/f4sJNl507N1evragbneQ368NIypUA3nfNjvfx+tX5z0rd1z6l3dllI8tx9r9MH5Qeo5M7FqtkJStg75/To0kodMi03svHykhYcu0CZqV2PzUzN1IJjQw81AgZd0FCiUCWrd9YPyQOZ93rc3hM0Kj1Ff3kjyqIifTyn8ejx5Ru0ZXurzjui7+GhweZO9YZ4Lx6EXrVwFwSufPaqXWYOc6JqbuvQN0uXyiQ9du6VKrukbMdaZN+cc7b+/I1DNTN/tL794Bv61t9u6fo6WFGq59+t0um3v6L12WNDP8BghKkQQyN7G/qLxEZQi0JxXqZaOwKU6O+Hl9/foua2gI4LKsvf3ZypudrW3K53Nm4b2IN1+/CStqFSV/3911o0ab2+emixSmaUdFlI8s7TL9HvvnygVq6v12m3/U83/++PcX1Vf0lZrdZVN+r0gyIfrlmyf4kWnrxQRTlFkkzJgXG6+rCbWEQTwyfEUCI1Nqr6ou/p4oeW6cLHL98pPyQPZFhxemqyphYu062rj4/u902Y5zRe57o453TnS2u196RROmy3vufWBps+MVs5GakDHv4Y6oLAuX8/X4fedK0qt1aEvA9DxHceP/nH21q1Yat+fcaskBcyczJSdd/X56pg8lLd9uYVXV4HX/vb13Xan67Xhromrb3kKrmhClMhhkb2NvQXiY2gFoUiv0R/GSX6I/bs25uUnZay46poKHP8fa99OMArpiE+vKS3tajghp+Fvctn95+kB8+fq7UNT+uSZy+Mz6v6fi/h7Glj9fLt5+qkt/7br7uX7O8F1K3fb9E+dp/WVRwwRA0FQggzZGhM9SYtq6jT1vaNoe8W4YfkWAybbGrtUEZS6ItPkQwrLl1Zqmc2/ETttjm63zcDHZ41zF54b4ve3bRd531iar/LkCclmeZMHfh6aqF6QFsDTVq29TaNSQ89h5kh4juHvy9brwcWl+uCo3br9aJwxohklbXf1aPAVodaZKMe0nOXH6Mjf3yxbCjDVC9DI4FgBLUoFI/1rrIwTy0ygYDTojWbdNRe4zQiJfxLbsroDE0ZnaHX1g1wnlqUH14OKspV+8gH5BSH1RG7VaCaUr9ZaRdeENUQp+z0VJ19WLH+tWqT3t00wN5LIEKuIHQPcFJRoZ6//BgVDWAeZSyGTTa3dWjefUuU2VyitOSuy41EOqx4/qL5au7oOjKjP79vwj2n7VMim7s63O58ca3GZ6fp5JmTo7r/IdPyVFbdqI/qox/NEi74t7rN+u2Jv+wxRDzF0hkiHs+CSugffMyBumTz6/rep/fs824VYXpPq5s3fPw5hTCFOEBQi8KE7HSlpSRR+TFCyyvrtGV7qz7VyxWuTnOm5uq1D2vkXPTz1AJhPrxEMrb8o+2h54rEfOjLIA9xOufwqcockazf//f9QWgc0LfF512mxl5KsoeaR5maFNmH5OFeH7G1PaBvlb6hF9/bot+depHu+twdXea9Ljx5YUTDigdajfWNCy7v8Zw2paZp/sFf0pMrwxdLiIU1G7fqxfe26GuHFfd6wa43c6d61YEXr43uYl57R0AjU3ouDyN5FwSCh4ibTGPSJiun5VualvmZqB4PQyzEBczvPvQrpTz0YJ93pcAWdhYEtSgkJXkl+tcx9DEiz769SSlJpqP37FmWv7s5U3O1ZXur1m6JPgT/56sX9/qBsDdx+8t7kIc4jckaoZK5hXr8zQ1ccMCQq9rWom8EpmvhV66UKywMOZSo+4fk7JRJymn+lgrTP93n+Ydz+Ym2joC+++AyLVqzWT89dT+dcXDBjmHFnfNeI537OZDfN845XZ05U785/XtdntPGW2/TO8eeogtL39Ajl1yvQFFRXFSEvOvFD5WRmqySudH/Lt170ihlp6dENU+tvSOgyx55UyMav6zUpK6Vf4N7QIN/lhsuK9f0UZ/VtU+sUntHIOp2x4U4rQ46kCHL7qqrepbQb4rsAiYFtrCzIKhFqSgviw+4Efr325s0Z2qucjJT+zx2ztSBrafW2NquK9L205/OviqqseWhfnmbS9PZ+10Z0eMP2TyZISjne94R05SSlKTbn18b9TmASPzkH2+rqbVDJ11/maysLOxQouAPyRsvL9eBY0/RRQ8t73OoW1566KF0yW6crn1ilTZtbR7YezNoeNW2ifka8fCD+tFJ++irhxRFfo4QQv2+SUvOiOjD4vPvVmn1R1u1x6Xf6PKc5s07Rw9/41D9pn2VTrz1GiWVl8e8IuTmbc16bPkGnT47X6MzR0R9nuQk09wo5ql1BJwue+RNPbZ8g6497gLd3a3yb7ge0PTUZF194j5as3Gb7n+1LOp2x1ycVgeNaMhyt4DpSku1orJO1z25Wq489PDFSC5gdr8w1J+ecGA4EdSiVJyXqbLqRgUo0R9W6cpS5d9YqH9vPUbP1JwW0QejaWOzNHbkCL0eZVC775Uy1TS0as7V34lqbHn3X9752YXaN+N7+vPzxXq9j7lzQzpPZsECtaZ1vQo80ApUE0al6/TZ+frL0kptrG8eYAOB0P67ZrOeeHODLjxmN+0+Pjvi+2WOSNHvv3KgWto69O0HlqktTI/Gbc99ILf1TCV3Wx8xIyVDn5pyse59pUwzf/Ujnf2386J7b3YbXpW75SPd+OzvdG7Z/yL+XsLp/vsmzSZot5RL9cV9vtTnfW9//gNNHJWuU2dN6bFvREqS/u/R3yuzvet821hVhLzvlTK1BQI65/CpAz7X3Kl5+nBLgzZtjex3VkfA6bKHl+ux5Rt0+fF76VvH7N6vHtDj952gT+w+Vr9+9l1Vb28Je1xci3V10DC9eVctCr0cwsVPfl//WrVR79+0UIHzz+8SMJvP+bru/ObPdNdLH6omL8x0iggvYEbbEw4MJ4JalIrystTSHtCmbXzADaUztKzfXiGZU3Xzhog+GJlFX9mrsbVdC19YqyP2GKuDisZE2/Quv7wrLi3Toguv1qScdJ1z9+taVl4b9n7h/ugMxjyZ5jPO1DUnXqTqsRMHtQLVBUftpg7ntPAFetUw+Bpa2nX139/S7uNH6ptH79bv++82bqR+cdoMLS2r1S+fXtNln3NO1z25Wtc/vUZf3q9Ed53SdZ7YHafcoSfP/4H+e9nRasq4X+2u6+/qiN+bIT7kprY0D9qH3ODfN4+ftlwNdYfqz0vC9BT43iiv1atra3TeEVPDz/eKk4qQTa0duv/VMh239wRNHZs14PPNneaNuuht+GNw7+mY66bo/pWlO0Jaf5mZfnzKPmps7dANz7wTdbtjpSMQfvHmsIs6D6YQvXnt552ve7+9QOX1oV/nW5o26Bv3LVX6NVcrqalrb3pGW4uuW/KQllx9nMbefAPrkWGXR1CLUrFfon/dFuaphTKQyf0HF+dqfV2TKmv799ze/2qZqhtadfFxe/Trfn0Zn52uB84/RHkjR+isP76m8t/e2eXqYOPd9+qOF9aG/aMzGPNk/rVqox7c4witeWXloFagKsjN1OdmTdYDr5XtvFeLEbdueOYdra9r0i8+v7/SUpKjOsfJMyfrrEOL9Jv//VETflmgpGuTVHRTkU6+6xf6wwtr9dVDinTTF2fpa7O+EvLqeGFepra1DaD0/zAGnk/tM0EHF4/Rb559Tw0t7WGPu/25D5STkaovzeml52AIhkv3i9+Lkp6eqidu/Iq+X/PGoJx2n0mjlJ2WEvZiXveRDdvaP9LW9N9p9NjFUT/m7uOzdfZhxXro9QqtqKyL+jxDKsQQwUWrN+mEm18Iu3jz5tHj9e+3N3nFu4ZqDluICx0pzU361P23KDtMYZcpo/L1j+98QlO2bQm5P2vTBm8ILeuRIQEQ1KJUlOddxWGeWmgDmdzfOU+tr6GGwRpb2/WH5zt703Ijvl+kJuZ4Ye201c9r3GXf7nJ10L4xTyt/9XtlJoUuljI+s+fQpP56YHG5CnMzdei0/i0SG4kLj95dLe0B/fHlDwf93Ehcy8prdc//1ukrhxRqdvHA3pO7FS1T3YhbtbmpUk5O5fXlerLyWh2+/2r95HP7Kimp9zW5BlK0I5AffRXZ/jIz/eCze2vL9hbd8WLoXu73N2/TM29v0tcOK1ZWWkr4ky1Y0KO3oT09Y3h6G7oNF83fWqXd5l8yKB/+U5KTNLt4TNgetVAXCdsCzQMe2XDRcXsoLytN1zy+Kv6mPITotWo59+t67IpfqbU9oKorr+mxeHN7eobu+Mx5Ou/eJbrl69eqo9sQw0GbwxbmgsbErVW67ZRfhSzocf2nrtN+U3JkkVxsoIQ+dnEEtShNHp2h1GSj8mMYA/lgNH2iV9mrPwVFSl8tH5LetGBTRmfo6v/dr4y2rj1PGW0tumHZw1p46o09/ugkKU1u65l6+q3QV/QjsbZquxZ/WKMz5xT0+YE0GruPH6kf1i9TyRePkouzimDYyQQV3Zg8a2999YOXdcVnpg/4tNc890N1dFvf0FmLXq66NaKFk0MV7YhkfSznnP508ryoq8hG48DCMTpx/0la+MJabQ4xtP7259cqPTVJZx9W3PuJgnobnJk2jp6gm8/4ntyXvzwk7e4iRC+KDeKcqEOm5WltVUOP56esukFlQ1QBNDs9Vd//zF56cf3fNOGGgvBFaXrrmRrGXqv01hb9fMlDevbSo3TAD77VY/HmlDvv0Pfv+4l+dup+OuOvtym52xDDwZrD1p4fek0/K+y5HEKPgh4hLjYwtBGJhqAWpeQkU0FuJj1qYVww62qZ6/rhJtLSt8lJpoOLI5+n1tjarj+88MGQ9aYFS1kfep21ERsqQ/7R+cNJC3XYpFP1rQfe0N+Whb5vXx56vUIpSaYvHDREi9iWlurse36uyfWbZXFUEQw7mW69KBNqN+maJ27SqL88POBTD7T8frj1sTLbj+r1fn9btl7XZs/SS1f8fFiHV11+/F5qbQ/opn+/12X7hrom/X3Zep15cKFysyKonuj3NlggoGeffFW/nTRXyyqGYejeEA8XPf7N/+il287RuJxMqbhYm27/oy59eLk+eePzSnHjQt5nMJZYaUl9QXUjbtWWpvWhi9L0Vl1xKCsvhnleR27aoNTk8Is3pyYn6SuHFGni1qp+nTdSzjnddcL5akoNf6Gj14IeDG0ECGoDMTUvix61MFavnaECu1gFowqjKn07Z2qu1lY1aEsE86ZKXy3Xlu2tuujYoetN26GPcfGPUwAAIABJREFUoRjd/+icd9BZuv/rczWnOFeXPvymXr72ln5dUW1p79CjSyt13N4TND47vddjozZ/fo8J27GqDoedWIir+snNTYPyOhqM9Q2D35tVV1TqkwWn6Yd/XxX2YltFTaN+9NgqzSnO1bE/vWRYh1cVj83SVw4p0p9fr9D7m7ft2H7ni97w5POO6H/1xM8fmK/s9BTd/fK6wWpmWIGCIRwuWlqqoh9crPytVTsuLGV/50LZgw/o7MOKdcsJ1w/Z+lhX/3d+j57dxrZGXfj45bruydXadtkVIasrNl1+pZouv3JIKi8GAk5bx00KvTPC5zvcEEMX7ucYocff3KDrxhyoV668LvqwxdBGJDiC2gB0rqXmXJyNV4+x19fV6Pl3q3TV0eer/JKyqErfVgcWqTLtHI2/MaPXNY+aWjv0hxc+0Cd2HzvgeTARiWIoRlZaiu4+52BdWfOGDvjZFf26ovrs25tU09CqM+cM7A9mr+KkOhx2ckP4OhrsxWmTk0y/OXOWzKTvPrS8R+n/9o6ALvnzcpmkX39xppKHYMhxX77zyd2VkZqs65/2Kg3WNrTqwdfKdcqsycofk9nHvXvKSkvRF2cX6KmVHw3pchwt7R267VPnDt1w0fnzvWGUwadub9Evl/5ZPzxpH31z7tlDtj5WuB7cre0bdff/1ilr00ch96d9tF5pH60PuW8glRdrGlp19j2v6+o5X1LriAEs3xLi71pjSpruPWmemlo7ompb9fYWXfvE25pZMFpH/fhiwhYQJYLaABSPzVRja4eqqJa3g3NOv/rXOxqXnaazDi2O6hylK0v18/9doo6kqj7XPCpdXOb1pg3h3LQuohyKkZ6arHlP39nvdY0eeq1CU0Zn6Ig9Qg/nGRSxrg6HXcMQvo6GYnHaKaMz9IvPz9CbFXW66d/vdtl323MfaElZrX566n5RhaLBkDcyTd88ejdlPPKQWqYUaHR2up695SxdvmVp1Of82v+3d+fhUVXnH8C/ZzJZZrJOFkKSyUyIO8oeEYutuLTSImBd6pJaQZTW1oVaxSVUUUtVpIBUy0+gVpQoKm6ItUhR2ooKJGHfETJZSEJCdrJnzu+PmUASZrLN3LmzfD/Pk4fk3jszr87Nzbz3nPO+P0hDu5SKNW+2WiX+8N5OvBSfgd1Pv6TMlDUniU1Q0Zmqu0r1x3I2gmuONuHgcxMBk+Mbam0pKWhLcVxUqjxmEP61p9RWoKS3NWyd9rcYU7Hk7rn47vuTGDfnQQS/vsK1UatOf9ekyYSvZ/8Zc6NG4sal3+Dksn/0e23ds+v2oa6pFfNvGq7KjQ4ivyGlVOVrzJgx0tdtOnhCmh9bJ7ceO6l2KF7jv4ds/0/e2HxswM9hXmSWmIuzvlIXmk4fs2rXKpm60CQxV0j9c4Plql2r3BC9woSQ0jaW1vVLCIeH51fUS/Nj6+SSfx9SNq5Vq6TU67vE1Bqms20n6qtVq2znTedzW6/3+vNo9vs7Zdrj6+TmI+VSSim3F1TJ9Cc+kw+8nadyZFI2v/GmbAgOdev/03tWbpOjnv1CNra0uTFSKa1Wq3z6kz3S/Ng6uXTTEbc+dxdms+PrqNms3Gvardq1Surn6bv8XdLP05/5++PgWnr6/XJynX3mtiel+bF18vnMOT3//jh4fENwqLQsWabYf++XB8rk7Btn9/sc/Pe+Uml+bJ1ctOGgYrER+RMAOdJJvsQRNRek2Uv051ewoAhgS/oXrD+IlBidS1P1nE0vKawpxISXvsJ1r83D3R/fi8LaAgASDe2lfWqmrbp+jji8u60QGgHckqHgtEfAQXW4QfjrrY9yeorCOjfl7Wl6r8/IzMQrt81GaUwipA8t/H9q8lAMiQvHr95eCONCE0a9Hovi0OkYfv5utUNDyNN/PKvKrKvrmqaPT0PlqRas3Xncxei6Wvqf7/HGN/mYccUQ/PpH6W597i5UrATY68huTzMuHOzTrliOJ1c9h0W3jsC0z5ZD23T2WuGa3z+KP/9zP2p+/+hZa9x0rc0w/UW5/+6rLhiEeVvf6dc5WNfUijkf78EFiZH47YT+Nxgnoq6YqLkgJUYHrUbAwoIiAGzrqXYW1eCha84bcHNbwPn0EkNYEi4YHIlNpUvQYu36B62vzbRV5WQdQM7MR846tLXdivdyinD1hYkYHK1QEZHOOlWHW//ZFrw8eCxyLX1vj0D9070pb0/Te33FobI6LE68FGs/2QzhQ2tRwkO1+EnGERxtX4jiukIAEs04gYfW36f++6HAur/L0+NwQWIk/rE536X11Z1vNCTMN+KpDf+HqSOTkfWzi/rUMmHAVK4E2Ou0yp6KXzjYpw3S4OejjEisOeHw9SLLS/HWtxZEljtp8aLwWmJnlY6dra178V8HUFbbhBdvHo4QLT9iErmKv0Uu0AZpYDTocIwl+mG1SizccAhD4sNx42jXGjw7Kxzw10nz8dqdGWiF41LCrvbJUVy3DxhWkwkr7nwct9anY8O+si6Hbtxfhor6ZtyuZBERJ27JMCJaF4zl/2UDbKU4asrrEzcbevD2lgKEBGlw02iF2kgo6P92/AlSnF3NT/X3Q4F1f0IITB+fhv0ltX1ugdJd9xsNFY3FqA59FaMv3KtIr8ez+GElQGeVFzVmE/Y/NxEas0priZ08f0lUAhasP4jyuuYu/RN/c9c1+EvrXoxMjVE2LqIA0edETQgRJITYLoRY52BfqBDiXSHEESHEFiFEmjuD9GYdlR8D3brdJThQWodZ154HbZBr+X9v00vcUapbNZ0+YGgsFty9dA4uSYnG77Lz8PXhitOHvbO1EEnRYbjyfAWLiDihD9Hil+NMWL+vlNN6FeJqXzBv09Tajg/zinDdJYMRFxHa+wO8jNe+HwpN87thVApi9MF4Y4Cl+h3daGiXTXh60xyX4gpovb3Xak35dPC67TodPr/9Aby66Qiev/OPaJlxz+n+icbactyw9Bn24SRyk/58on4IwH4n+2YAqJJSngtgEYAXXQ3MV6TF6WGpaAjoEv1t7VYs3nAIFw6OxOThyW55zp6ml7i7VLeaIkK1WDn9UqQnhOPeN3NwdPEytKWa8I8Z47B+8a+gXf2OKnHddXkagjUa/P1rjqopwadvNjjw2a4S1Da14Y6xvhm/174fCk3zCwsOwu1jTfhiXykKK/s/dd9rE1tf1tt7rdaUTwevG7R8OWYsnYN/P3wl/rh5FUKau7Z7EOzDSeQ2fUrUhBBGAJMArHByyFQAK+3frwFwjVB0krr3MMeFo665DZWnWtQOxfPs0x2CgrV487lfYH7zHo9Me1GiVLeaYvQheGvGZcj8/mskzX4Q2qJCaCARdeJ4r33WlDIoKgw3jErG+7mFqArEc1thT1/5HITsOvLUr5sNvZXx9rC3txYgPT4c49I90MtQAV5980ehaX53jjNDCIG3BlCq32sTW1/X23ut1pRPJ697TkIEDCfVWTtHFCj6OqK2GMBsAFYn+1MAFAKAlLINQA2AuO4HCSFmCiFyhBA55eWO1xn5mrR4e+VHfywo0tOHwexsWxLRabrDsGcf9dgHRqX65KglITIUj//vTbdXeHPFPT9MR1OrVbGeS4HMgKsR23o/EvVGAAJ6zeC+32zo9LvX18bp/dHfapQHS+uQa6nC7WNNyhaRUJC/3fzpi+QYHdJN2zF364R+Vx698ZxHXLvRQP6DfTiJFNVroiaEuB7ACSnlwLts2kkpl0kpM6SUGQkJnl97owRzXDgA+N86NScfBptXvoVDZXVofPTxs0oFc7qDa5xV11LrzuT5iZGYcEECVn6bj6bWdlVi8FdrcotwUfTPcPwPBXj1ykNIOLUCYwdN6duDs7LO+t1zV0I/kGqU72y1FxEZ43tFRDrzt5s/vcnenY2vK+ahVZzoV+XRwsoGbMg5D2OiHoMp2hQwiS05oWK7BKJA0JcRtfEApggh8gGsBnC1EGJVt2OKAaQCgBBCCyAawEk3xum1jAYdNMIPR9ScfBgsf/AP+Mmi/yK0pNjx4zjdYeC88M7kzB+mo6K+BR9vd/J+U78VVzdi8/cVuHl0KjQagVszUhESpOn7FDQnv2OyoADH7MVfBtqj7dEvnuhXNcrGlnZ8kFeEiZcMRmx4SN/iJ6+QtTELTe39a3PS1m7FQ6u3AwA+nPYYLLMsAZPYkhMqt0sg8ne9JmpSyieklEYpZRqA2wB8KaX8ZbfD1gK4y/79zfZjAqK6Rqg2CMkxOv8bUXPyYTClrgIv3zYSrSlOSvBzusPAeeGdycvPicPFyVFY/r+jsFoD4ldacR/kFkFKnG5jERcRiuuHJ+HDvGLUN7f1+niZ6rhlQ3FkPK5asAmXLZqLuz+51/moWLcpzdZV2fhibyluW/YtSuocj+o6KxLx2e4S1DW14Y7L+HvvawZSEGTJl0eQV1CNP/38EqTG6p0eRwHGD9slEHmLAddRF0I8K4TomKvzdwBxQogjAB4G8Lg7gvMVQ+LD/W9EzUnCJUwmTB2ZgtAXX/C6pMLneeGdSSEEZv4oHd+Xn8KmQ44bslLfSSmxJrcIPzgnrssH3TsvN6O+uQ0f5TmZ/trJvvsfR4O2Wwl8vR6Rf5mP2RMvwM66pWhxMFIy6/PHcHjRa7Dee2+XKc0td8/AuscXoLCyEbFhSQ5fMynC8bTGt7dYkJ4QjsuG+GYRkUDmrPBHuDYRpxzcMNh6rBKvfHkYN45OwdSRrvXKJCKivulXoial3CSlvN7+/VNSyrX275uklLdIKc+VUo6VUh5VIlhvZY7T+9+I2rx5kD0lYl6YVPgFL7wz+bNhSbjr6GYMGz/CaZXBgU61CzRbj1WioLIBt2R0TXxGpsZgWEo03vzW0mOrj7Z2K2YFDcXCWx6BNJm6/O5F3zMNv51wLlqk40JNFQ3HoZv7R2gauyZxYa3NeCF3Nf7z6AQsmTT/rOqHGoQiuP4OHCyt67L9QGkt8gqqcYcPFxEJZI4qXYZodAhryMTP/7b59DRaAKhpaMWs1duRGqvHs1Mv8XSoREQBy7XOxAQASIsLR3VDK6ob/KiMeWYmKhe9gqKoBEhniZgXJhXkfsGr38GctYuRUFnmsMrgQApQBKr3c4sQEarFxIu7jlwJIXDn5WYcPlGP745WOn38ezlFOHyiHhlP3g9hsTj83XM2UpISaURKXYXDffrS49AGaRxWP1xw7d+QGHwt7lj+XZdk7Z0t9iIio327iEigcvRev37Dcnw07XGU1zVjyitfI2v9UpgXmxHzUii2Nd+OiZceQUSoVu3QiYgChlBrKVlGRobMyclR5bXdbcO+Mtz7Zg4++d14jEiNUTsct/l4ezFmvbsDnz14BS5OjlY7HFJLWpotOevmhCERdz39PjbV3IImWXbWfnO0Gfmz8pWPz0ecam7DpfP+jSkjkvHCTcPP2t/U2o5xz2/ED86Jw98yx5y1v765DRNe+grp8RF499fjnI5idSTOnYuC6IP1tqp8k7Mcvpcwm20JnxNHy+tx27Lv0G6VuOOqAry87VkU1hQiMngwlk55iYUk/ExhZQOuX/ECdjfMhxRnWoacPo/4fhMRuY0QIldKmeFoH0fU3CAtrqOXmn9Nf8yxVCI8JAgXDo5SOxRSk5PCMglVJ5ASo0OTdLx2raeiBIHon7tL0NDSjpudlLEPCw7CLzJSsX5vGUprms7a/9p/vkdFfQuenHRRj1MNe+wJNsCCNekJEVg9cxyq5Jd4dOPvUFhbAAiJurYSjp76odRYPaqCV3ZJ0oDeq0ISEZF7MVFzg9RYPYQALH5WUCTXUo1RJgOCNFx/EtCcFZYxm7DirgyYnUy1czYFL1CtyS1Cenw4xpgNTo/55WVmWKXE21u7JrklNY1Y/r+jmDIiGSP7MGrvtCeYC2tL0xMi0ByeDQl+eA8ERbWFDrfzBgwRkecwUXODsOAgJEWF+dWIWl1TKw6W1vb4oZICRC+jMI6KEgRrwjDvGlYA7VBwsgFbjlXipjHGHkfDTHF6TDg/Ae9sLUBLm/X09gXrD8FqBR697gLXg3FhbWlJff/K95PvcnajhTdgiIg8h4mam5jjwv1qRG1HYTWsEshIY6IW8HoZhek+1S5COxiGlvsxKm6yyoF7jzW5hdCIM73TevKry9NQXteM9XtLAQB7imvw4fYiTB+fpnrvKn54DxyObsDog/W8AUNE5EFM1NwkLd6/SvTn5FdBI9CnaVYUAHoZhek81c4yy4LU0Ovw8Hs70NzWrkq43sRqlfggrxhXnJeApGhdr8dfeX4CTLF6vGUv1f/nf+5HjC4Yv73qXA9E2zN+eA8cPa51JCIij2Ci5ibmuHBU1LegrqlV7VDcItdShQsGRyEyLFjtUMjHxIaH4IUbh+FAaR2WbDysdjiq+/boSRRXN+IWJ0VEutNoBObW7cDCx6YCQUGY/8gULG7fh2id+r+L/PAeWJyudSQiIo9gouYmHZUf/WH6Y7tVYntBFcaYOZpGA3Pt0ETcMsaIpZu+R15BldrhOOWJRt3v5xQiKkyLHw9N7GNQ2bjqL3NgrC2HkBLG2nL8aEHWWU3G1cIP70RERJ7BRM1NzHHhAPyjRP+B0lqcamlHhjlW7VDIhz01eSiSonV45L2daGzxvimQSjfqzt6dDdMiM14+MAaWkOn44MDqvj0wKwuisesNH9HQAGSxsiIREVEgYaLmJh2L/IuqGlWOxHV5FtsICCs+kisiw4Lx0i3DsatqHZIWpCo6ajUQWRuzujSFBtxXar4jCezoN1bb2o9+Y0761jndTkRERH6JiZqbRIRqYdAHo7DS96c+5liqMCgyFEZD74UPiHpyrGE9akNfRU1riSKjVq5wVlLeHaXmXUoCnfStc7qdiIiI/BITNTcyGvR+MaKWa6nCGLOhx35PRH2RtTELrbKpyzZvaJB85EQ9gpHgcJ8xKtXl53cpCeylbx0REREFBiZqbpQaq0NRlW+PqJXVNqGoqpHTHsktlBy1Gqi9x2tw62vfIllMR5i266ixkKEwBd094DV1Ukq88uVhaKzxDvf3qd9YL33riIiIKDAwUXOjjhE1KaXaoQxYrn19WkYaC4mQ67ytQXJeQRVuX/YdQrUabLxvDlZMWd6l1PwDo15CcUkGpr+xFaea2/r13PXNbbhvVR4WfHEI1yY/BJ3WhX5jvfStIyIiIv/HRM2NjAYdmtusqKhvUTuUAcvJr0KoVoOhSVFqh0J+wFGDZJ3Wcw2SO5ffH7wgFZP//jwM4SF47zeXIz0h4qxS8y9PfQCLbx2JrccqMe0fW/F63lvOy/dnZwNpaYBGg9ZUE5bcPRcb9pdhzqSL8PnMJ7F8CvuNERER0cBp1Q7An3QU3yisakBCZKjK0QxMrqUSI1JjEKJlDk+u60hMsjZmoaCmABprPKYNzfJIwtJRebGjqEfZqSJoNEsw50dDYTRc5fRxU0emQKvRYPp7C/FR2V/RjmYAOF0IBQAydwGYORNosD13cFEhfv/+Atz8wmKc/8Of2Y4ZlsnEjIiIiAaMn8bdyGjw7RL9jS3t2Hu8luvTyK06Rq3an2rHhOj3UVk+1iOv66jyohXNePHbub0+dtLwJIio1aeTtA4NrQ349SePoPzBR04naR10rc04f8kLLsdNREREBDBRc6uOETVfLSiys6gabVaJDCZqpAAhBCYPT8aWY5Uoq23q/QEucrWQSXljscPtp9rLEFdZ5uRF2euMiIiI3IOJmhvpQ7SICw/x2RG1jkIio01M1EgZ149IgpTAZ7tKFH+tlEijw+19LWTi7DhztAkaM3udERERkbKYqLmZ0aDz2abXuZYqnJMQDkN4iNqhkJ86JyECFydHYe3O44q+TkNLG+Lap0HIrmtF+1N50VEhlNOPZ68zIiIiUhgTNTczGvQo9sERNatVIq+gChlmluUnZU0ekYwdhdWK3dCwWiVmrd6B2srL8MTliwZceTFzWCaWTXZSuZG9zoiIiEhhrProZkaDDhv2l8FqldBohNrh9NnRinpUN7SykAgpbtKwJLzw+QF8uus4fjvhXLc//4vrD+CLfWV4evJQTB8/CfOuu2/Az9Vj5cbMTCZmREREpBiOqLmZMVaPljYrKuqbez/Yi3SsTxuTxkSNlJUaq8doUww+3en+dWrvbSvEa/85il+OM2HaD9Lc/vxEREREnsJEzc3O9FLzremPOflVMOiDkR4frnYoFAAmj0jG/pJaHDlR5/qT2RtPS40G468dg9knczB38sUQwndGtImIiIi6Y6LmZqk+WqI/11KFMWYDP9ySR0walgSNgOujatnZtsbTFguElEipLcd92fOhXf2OewIlIiIiUgkTNTdLifG9pteVp1pwtOIURnN9GnnIoKgwjEuPw6e7jkNKOfAnyso6q/G0aGywbSciIiLyYUzU3EwXEoT4iBCfGlHrWJ/Gio/kSZNHJONo+SnsK6kd+JM4azDNxtNERETk45ioKcBo0PvOiFp2NsZeNRpHX5yMjAmjbFPJiDxg4sWDodUIl6Y/thkdN7Vm42kiIiLydUzUFGA06HwjUbOv74k+cRwaSGgKCmzrfZiskQcYwkPww/Pi8enOgU9/XPPz+9Cg7drUmo2niYiIyB8wUVNAR9Nrq9WFtTee4GB9Dxq4voc8Z/KIZBRXN2J7YXW/H3u4rA5P6ofj8wefZeNpIiIi8jtM1BRgNOjQ0m7FiTov76XG9T2ksh8PTUSIVoO1O473+7Hz1x9EeIgWVz07C8jPB6xW279M0oiIiMgPMFFTgNFXSvQ7W8fD9T3kIZFhwUhNzsXz26+G5hkN0hanIXt371Nvc/IrsWFfGX4z4RzEhod4IFIiIiIiz2KipoDUWN8o0V//1DNc30Oqyt6djc2Vf0YLTkBCwlJjwcxPZ/aYrEkp8fznBzAoMhTTx6d5LlgiIiIiD2KipoCUGNuIWmGld4+ofXTRlXh84v1oSUnl+h5SRdbGLDS3d72h0dDagKyNztdJbthXhlxLFWZdez70IVqlQyQiIiJSBT/lKCAsOAgJkaFeP6L20fZinLp6CoLXvmRL1Ig8rKDG8XpIZ9vb2q2Yv/4g0hPC8YsMJ6X5iYiIiPwAR9QUYjToUFTtvSNqBScbkFdQjamjkiGYpJFKTNGO10Mao1Idbv8grwhHTtRj9nUXQhvEyxcRERH5L37SUYi3N73+ZEcxAGDKiGSVI6FANu+aedAH67tsEzIUkc134nBZXZftjS3tWLjhEEaZYnDdxYmeDJOIiIjI45ioKSTVoMPx6ka0e2EvNSklPt5RjLFDYmE06Ht/AJFCModlYtnkZTBHmyEgYI4246krXoaubQKmvroZn+woRvbubKQtToP++WDkNt+Bkefv4SgwERER+T2uUVOI0aBHa7tEWW0Tku3FRbzFnuJafF9+CjOuSFc7FCJkDstE5rCuBWzuG9uE+9/Ow4z3F6E29FW0yiYAQLumHPO3PoxzEyPOegwRERGRP+l1RE0IESaE2CqE2CmE2CuEeMbBMdOEEOVCiB32r3uUCdd3nOml5n3THz/eUYyQIA0mDUtSOxQihxKjwvD2vePQFvH26SStQ29VIYmIiIj8QV+mPjYDuFpKOQLASAAThRDjHBz3rpRypP1rhVuj9EFubXqdnQ2kpQEaje3f7N4bAjvTbpVYu/M4JlyQgGh9sOuxESkkOEiD2pZSh/ucVYUkIiIi8he9JmrSpt7+Y7D9y/sWXnmZjumOLo+oZWcDM2cCFgsgpe3fmTNPJ2sd63c0z2iQtjitx0bBAPDN9xUor2vGDaNSXIuLyAOcVYV0tp2IiIjIX/SpmIgQIkgIsQPACQAbpJRbHBx2kxBilxBijRDCYW1tIcRMIUSOECKnvLzchbC9X1hwEBKjQl0fUcvKAhq6PUdDA+STTyJ7dzZmfjoTlhoLJCQsNRbM/HRmj8nax9uPIzJUi6svHORaXEQe4KgqpD5Yj3nXzFMpIiIiIiLP6FOiJqVsl1KOBGAEMFYIcUm3Qz4FkCalHA5gA4CVTp5nmZQyQ0qZkZCQ4ErcPsFo0KOw0sURtQLHU7xkQSFmfPQwGlq7JnE9rd9pam3H+r2l+OmwwQgLDnItLiIPcFQVctnkZSwkQkRERH6vX1UfpZTVQoivAEwEsKfT9pOdDlsBYL57wvNtRoMOeQVVrj2JyWSb7thN/aAkNMsShw9xtn7n3/vLUN/chhtGctoj+Q5HVSGJiIiI/F1fqj4mCCFi7N/rAPwYwIFux3QuHzgFwH53BumrjAYdSqqb0NZuHfiTzJuHlpCwrtv0ekQtnA+zk3U6xiiHM0/x8fZiDI4Kw2XpcQOPh4iIiIiIFNeXqY9JAL4SQuwCsA22NWrrhBDPCiGm2I950F66fyeABwFMUyZc32I06NFmlSirax74k2Rm4s83/B4n4wcDQgBmM7BsGZCZ6XD9jpChiG+/CxX1XV+z6lQLNh0sx5SRyQjSsFkwEREREZE363Xqo5RyF4BRDrY/1en7JwA84d7QfF+qwZZEFVY2IGWATa+LqxvxxpDxSPv4XkwbP6TLvo7pYFkbs1BQUwBTtAl3XDgbH24egpuXfoM3774MpjhbDJ/tLkGbVWLqyGQX/ouIiIiIiMgT+rVGjfrHHU2vc/IrAQAZabEO9ztav3PTRVWYsXIbblz6DT6KyUfqgj/hDksBfmwYhEHD/gJkcr0PEREREZE361PVRxqYpJgwCOFa0+uc/CpEhGpx4eDIPj9mjNmANb+5HNfv/hLxD98PWCzQQCKxqgyiUw82IiIiIiLyTkzUFBSqDUJiZJhLI2rb8isxyhQDbVD/3qpzB0VizreroGvttj6uocHWm42IiIiIiLwWEzWFGQ26AY+o1TS24mBZHS51Mu2xN9qiIsc7nPRmIyIiIiIi78BETWGpsQNvep1XUAUpgYw0w8Be3OS4fL/T7URERERE5BWYqCnMaNChtHZ2uhokAAAIVElEQVRgvdRy8iuh1QiMTI0Z2IvPmwfou5bvh15v205ERERERF6LiZrCjAYd2q0SJTVN/X7stvwqXJwSDX3IAItzZmbaeq6ZzWf1YCMiIiIiIu/FRE1hRnsvtf4WFGlua8fOwmpcah7gtMcOmZlAfj5gtdr+ZZJGREREROT1mKgpLPV0ota/giJ7imvR3GZ12j+NiIiIiIj8FxM1hQ2ODoNGAIX9HFHLtXQ0unZxRI2IiIiIiHwOEzWFhWg1GBwV1u8RtW35VRgSH474iFCFIiMiIiIiIm/FRM0DjAZ9v9aoSSmRk1+JDFfXpxERERERkU9iouYBRoMOxf1I1L4vP4WqhtYBN7omIiIiIiLfxkTNA4yxepTUNKK1j73UcvK5Po2IiIiIKJAxUfMAo0EHqwRKqvvWS21bfhXiwkMwJD5c4ciIiIiIiMgbMVHzAKNBB6DvJfpzLJXISDNACKFkWERERERE5KWYqHlAaj+aXp+obYLlZAPXpxERERERBTAmah7Q0UutLyNqOZYqAGCjayIiIiKiAMZEzQOCgzRIitb1qen1tvxKhAVrcHFylAciIyIiIiIib8REzUOMBl3fRtTyqzAq1YDgIL41RERERESBitmAh/Sl6XV9cxv2Hq/BpSzLT0REREQU0JioeYjRoENpbRP2FNc4PWZHQTWskuvTiIiIiIgCHRM1D7lxdAqSosJw09Jv8GFekcNjtuVXQiOAUaYYD0dHRERERETehImah5jjwrH2gSswMjUGD7+3E3PX7kVru7XLMTmWSlyUFIXIsGCVoiQiIiIiIm/ARM2D4iNCseqey3D3+CF445t8ZK7Ygor6ZgBAa7sV2wuq2T+NiIiIiIiYqHlacJAGT00eisW3jsTOwmpM/uvX2FlYjf0ltWhoaUcGC4kQEREREQU8rdoBBKobRqXg3EER+PVbubjltW9x2RDbSFqGmSNqRERERESBjiNqKrokJRqfPnAFLk0z4H+HK5Aaq8Pg6DC1wyIiIiIiIpVxRE1lseEhWDl9LF7771EkxzBJIyIiIiIiJmpeQRukwe+uOlftMIiIiIiIyEtw6iMREREREZGXYaJGRERERETkZZioEREREREReRkmakRERERERF6GiRoREREREZGXYaJGRERERETkZZioEREREREReRkmakRERERERF6GiRoREREREZGXYaJGRERERETkZZioEREREREReRkmakRERERERF6GiRoREREREZGXEVJKdV5YiHIAFlVevGfxACrUDoL8Hs8z8gSeZ6Q0nmPkCTzPyBPUOs/MUsoERztUS9S8lRAiR0qZoXYc5N94npEn8DwjpfEcI0/geUae4I3nGac+EhEREREReRkmakRERERERF6GidrZlqkdAAUEnmfkCTzPSGk8x8gTeJ6RJ3jdecY1akRERERERF6GI2pERERERERehokaERERERGRl2Gi1okQYqIQ4qAQ4ogQ4nG14yHfJ4RIFUJ8JYTYJ4TYK4R4yL49VgixQQhx2P6vQe1YyfcJIYKEENuFEOvsPw8RQmyxX9PeFUKEqB0j+TYhRIwQYo0Q4oAQYr8Q4nJez8idhBC/t/+93COEeEcIEcZrGbmDEOJ1IcQJIcSeTtscXr+EzRL7ObdLCDFajZiZqNkJIYIAvArgpwCGArhdCDFU3ajID7QB+IOUciiAcQB+Zz+vHgewUUp5HoCN9p+JXPUQgP2dfn4RwCIp5bkAqgDMUCUq8icvA/iXlPJCACNgO994PSO3EEKkAHgQQIaU8hIAQQBuA69l5B5vAJjYbZuz69dPAZxn/5oJYKmHYuyCidoZYwEckVIelVK2AFgNYKrKMZGPk1KWSCnz7N/XwfahJgW2c2ul/bCVAG5QJ0LyF0III4BJAFbYfxYArgawxn4IzzNyiRAiGsCPAPwdAKSULVLKavB6Ru6lBaATQmgB6AGUgNcycgMp5X8BVHbb7Oz6NRXAm9LmOwAxQogkz0R6BhO1M1IAFHb6uci+jcgthBBpAEYB2AIgUUpZYt9VCiBRpbDIfywGMBuA1f5zHIBqKWWb/Wde08hVQwCUA/iHfYrtCiFEOHg9IzeRUhYDWACgALYErQZALngtI+U4u355RV7ARI3IA4QQEQA+ADBLSlnbeZ+09chgnwwaMCHE9QBOSClz1Y6F/JoWwGgAS6WUowCcQrdpjryekSvs64OmwnZTIBlAOM6eqkakCG+8fjFRO6MYQGqnn432bUQuEUIEw5akZUspP7RvLusYQrf/e0Kt+MgvjAcwRQiRD9u07athW0sUY58+BPCaRq4rAlAkpdxi/3kNbIkbr2fkLtcCOCalLJdStgL4ELbrG69lpBRn1y+vyAuYqJ2xDcB59spCIbAtXl2rckzk4+zrhP4OYL+UcmGnXWsB3GX//i4An3g6NvIfUsonpJRGKWUabNeuL6WUmQC+AnCz/TCeZ+QSKWUpgEIhxAX2TdcA2Adez8h9CgCME0Lo7X8/O84xXstIKc6uX2sB/Mpe/XEcgJpOUyQ9RthG+QgAhBA/g22dRxCA16WU81QOiXycEOIKAP8DsBtn1g49Cds6tfcAmABYAPxCStl9gStRvwkhJgB4REp5vRAiHbYRtlgA2wH8UkrZrGZ85NuEECNhK1gTAuAogOmw3fTl9YzcQgjxDIBbYauavB3APbCtDeK1jFwihHgHwAQA8QDKADwN4GM4uH7ZbxS8AtvU2wYA06WUOR6PmYkaERERERGRd+HURyIiIiIiIi/DRI2IiIiIiMjLMFEjIiIiIiLyMkzUiIiIiIiIvAwTNSIiIiIiIi/DRI2IiIiIiMjLMFEjIiIiIiLyMv8Pd8ZauOVzBT8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrDBAqN_sgBC"
      },
      "source": [
        "# First improvement"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rkmipdk7QKj"
      },
      "source": [
        "eval_callback = EvalCallback(env, best_model_save_path='gdrive/My Drive/stock/logs/',\n",
        "                             log_path='gdrive/My Drive/stock/logs/', eval_freq=500,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=500, save_path='gdrive/My Drive/stock/logs/',\n",
        "                                         name_prefix='rl_model')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvCuAbTyrvZX"
      },
      "source": [
        "env_maker = lambda: gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)\n",
        "env = DummyVecEnv([env_maker])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqXH9rVqszUD",
        "outputId": "f27bfdfc-ebec-476a-d762-a38fb94957e4"
      },
      "source": [
        "model = A2C('MlpLstmPolicy', env, verbose=1) \n",
        "model.learn(total_timesteps=100000, callback=eval_callback)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:420: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:160: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:196: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "---------------------------------\n",
            "| explained_variance | -656     |\n",
            "| fps                | 15       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 4.25e-05 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -0.0502  |\n",
            "| fps                | 217      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.00632  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.018    |\n",
            "| fps                | 239      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 0.0304   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.00871  |\n",
            "| fps                | 247      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 0.368    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.01     |\n",
            "| fps                | 252      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 0.0285   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0243   |\n",
            "| fps                | 256      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 0.0213   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0135   |\n",
            "| fps                | 259      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 0.000219 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500, episode_reward=-0.44 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0179   |\n",
            "| fps                | 261      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.0563   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0602  |\n",
            "| fps                | 262      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 0.00682  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "----------------------------------\n",
            "| explained_variance | -9.66e-05 |\n",
            "| fps                | 264       |\n",
            "| nupdates           | 900       |\n",
            "| policy_entropy     | 0.691     |\n",
            "| total_timesteps    | 4500      |\n",
            "| value_loss         | 0.00438   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.245    |\n",
            "| fps                | 264      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 0.000447 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0585  |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 0.00418  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.703   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.00759  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.359    |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 0.000955 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.13    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 0.66     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.000534 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.32    |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 0.668    |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 0.000293 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0347  |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 0.682    |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 0.0199   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0698  |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 0.612    |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.000172 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.591    |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 0.622    |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 0.000203 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.503    |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.0256   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0779   |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 0.00993  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.341    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 0.000978 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.394    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 0.0471   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.227   |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 0.00063  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -7.76    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 0.65     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.0201   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -4.28    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.0774   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.117    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 0.0616   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.697   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.000456 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.647    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.000742 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.142    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 0.0516   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -0.177   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 0.0133   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.153    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 0.00069  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.89    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 0.684    |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.00191  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0426   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 0.665    |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.014    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0599   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 0.661    |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 0.000442 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0856   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 0.679    |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 0.0113   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.297   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 0.000861 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18500, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.735   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 0.684    |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 0.000109 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.176   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 0.677    |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.016    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.02    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 0.658    |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.0112   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.2     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 0.00383  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -36.8    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 0.0135   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=-0.07 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0181   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 0.676    |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.00152  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0676   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 0.67     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 0.015    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.695    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.0253   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.93    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 0.000163 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -3.01    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 0.00096  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.000944 |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 0.0497   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.263    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 0.00601  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.524    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 0.00135  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=2.36 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -1.35    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 0.102    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=25500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -8.2     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 0.681    |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.0018   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.137    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 0.0408   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=26500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.00501  |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 0.379    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.208   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 0.682    |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 0.0685   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=27500, episode_reward=1.13 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.088   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.0327   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=1.31 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.303   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 0.00341  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=28500, episode_reward=2.39 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | 0.104    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 0.00318  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.702    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.0255   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=29500, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.263    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 0.00425  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=2.45 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -0.561   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.00558  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=30500, episode_reward=2.67 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -10.5    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6100     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 30500    |\n",
            "| value_loss         | 0.00125  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=2.17 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0531  |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6200     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 31000    |\n",
            "| value_loss         | 0.00729  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=31500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.23     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6300     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 31500    |\n",
            "| value_loss         | 0.0777   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0866  |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6400     |\n",
            "| policy_entropy     | 0.682    |\n",
            "| total_timesteps    | 32000    |\n",
            "| value_loss         | 0.0282   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.259   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6500     |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 32500    |\n",
            "| value_loss         | 0.00103  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.358    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6600     |\n",
            "| policy_entropy     | 0.653    |\n",
            "| total_timesteps    | 33000    |\n",
            "| value_loss         | 0.17     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=33500, episode_reward=2.45 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0292   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6700     |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 33500    |\n",
            "| value_loss         | 0.0107   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=0.74 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.849   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6800     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 34000    |\n",
            "| value_loss         | 7.19e-05 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=34500, episode_reward=2.18 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.767    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 6900     |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 34500    |\n",
            "| value_loss         | 0.0234   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=0.77 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.41    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7000     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 35000    |\n",
            "| value_loss         | 0.013    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=35500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -7.91    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7100     |\n",
            "| policy_entropy     | 0.677    |\n",
            "| total_timesteps    | 35500    |\n",
            "| value_loss         | 0.0329   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.159    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7200     |\n",
            "| policy_entropy     | 0.659    |\n",
            "| total_timesteps    | 36000    |\n",
            "| value_loss         | 0.156    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=36500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.707    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7300     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 36500    |\n",
            "| value_loss         | 0.00174  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.823    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7400     |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 37000    |\n",
            "| value_loss         | 0.000989 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=37500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -22.5    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7500     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 37500    |\n",
            "| value_loss         | 0.00773  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.324    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7600     |\n",
            "| policy_entropy     | 0.658    |\n",
            "| total_timesteps    | 38000    |\n",
            "| value_loss         | 0.04     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=38500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.121    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7700     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 38500    |\n",
            "| value_loss         | 0.0107   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0524   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7800     |\n",
            "| policy_entropy     | 0.664    |\n",
            "| total_timesteps    | 39000    |\n",
            "| value_loss         | 0.0144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=39500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -1.69    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 7900     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 39500    |\n",
            "| value_loss         | 0.00205  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.579    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8000     |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 40000    |\n",
            "| value_loss         | 0.000245 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.411    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8100     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 40500    |\n",
            "| value_loss         | 0.000314 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.675    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 8200     |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 41000    |\n",
            "| value_loss         | 0.00636  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=41500, episode_reward=2.59 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.711   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 8300     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 41500    |\n",
            "| value_loss         | 0.000728 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=2.66 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.138   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8400     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 42000    |\n",
            "| value_loss         | 0.00571  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=42500, episode_reward=1.72 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.199   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8500     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 42500    |\n",
            "| value_loss         | 0.0183   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=0.65 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.153   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8600     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 43000    |\n",
            "| value_loss         | 0.00432  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=43500, episode_reward=2.40 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.45    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8700     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 43500    |\n",
            "| value_loss         | 0.00555  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=0.61 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -550     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8800     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 44000    |\n",
            "| value_loss         | 0.0322   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=44500, episode_reward=2.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.903   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 8900     |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 44500    |\n",
            "| value_loss         | 0.000671 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.452   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9000     |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 45000    |\n",
            "| value_loss         | 4.7e-05  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=45500, episode_reward=2.45 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.9      |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9100     |\n",
            "| policy_entropy     | 0.68     |\n",
            "| total_timesteps    | 45500    |\n",
            "| value_loss         | 0.0257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.88    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9200     |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 46000    |\n",
            "| value_loss         | 0.183    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=46500, episode_reward=2.37 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0863   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 9300     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 46500    |\n",
            "| value_loss         | 0.00615  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.241    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9400     |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 47000    |\n",
            "| value_loss         | 0.114    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=47500, episode_reward=2.72 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.64     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9500     |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 47500    |\n",
            "| value_loss         | 0.00199  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.511   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9600     |\n",
            "| policy_entropy     | 0.674    |\n",
            "| total_timesteps    | 48000    |\n",
            "| value_loss         | 0.00455  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48500, episode_reward=2.34 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -113     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9700     |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 48500    |\n",
            "| value_loss         | 0.00483  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.247    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9800     |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 49000    |\n",
            "| value_loss         | 0.0912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=49500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.61     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 9900     |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 49500    |\n",
            "| value_loss         | 0.0025   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.698   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10000    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 50000    |\n",
            "| value_loss         | 0.008    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=50500, episode_reward=2.22 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.11    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10100    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 50500    |\n",
            "| value_loss         | 0.0123   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.567    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10200    |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 51000    |\n",
            "| value_loss         | 0.00128  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=51500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.04     |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10300    |\n",
            "| policy_entropy     | 0.676    |\n",
            "| total_timesteps    | 51500    |\n",
            "| value_loss         | 0.0107   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0582   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10400    |\n",
            "| policy_entropy     | 0.675    |\n",
            "| total_timesteps    | 52000    |\n",
            "| value_loss         | 0.00132  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=52500, episode_reward=2.26 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.842   |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10500    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 52500    |\n",
            "| value_loss         | 0.00888  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=2.02 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -5.65    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 10600    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 53000    |\n",
            "| value_loss         | 0.00054  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=53500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0879  |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 10700    |\n",
            "| policy_entropy     | 0.674    |\n",
            "| total_timesteps    | 53500    |\n",
            "| value_loss         | 0.269    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -189     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 10800    |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 54000    |\n",
            "| value_loss         | 0.000967 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=54500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.258   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 10900    |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 54500    |\n",
            "| value_loss         | 0.004    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=1.94 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.257    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11000    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 55000    |\n",
            "| value_loss         | 0.0485   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=55500, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0935  |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11100    |\n",
            "| policy_entropy     | 0.654    |\n",
            "| total_timesteps    | 55500    |\n",
            "| value_loss         | 0.0285   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=2.41 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0613   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11200    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 56000    |\n",
            "| value_loss         | 0.00457  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56500, episode_reward=0.70 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.39     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11300    |\n",
            "| policy_entropy     | 0.675    |\n",
            "| total_timesteps    | 56500    |\n",
            "| value_loss         | 0.0257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=1.64 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.24    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11400    |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 57000    |\n",
            "| value_loss         | 0.000833 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57500, episode_reward=0.81 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.42    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11500    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 57500    |\n",
            "| value_loss         | 0.00011  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=0.81 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.863    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11600    |\n",
            "| policy_entropy     | 0.665    |\n",
            "| total_timesteps    | 58000    |\n",
            "| value_loss         | 0.0358   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=58500, episode_reward=2.09 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.38     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11700    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 58500    |\n",
            "| value_loss         | 0.0058   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.32    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11800    |\n",
            "| policy_entropy     | 0.625    |\n",
            "| total_timesteps    | 59000    |\n",
            "| value_loss         | 0.0538   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=59500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0709   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 11900    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 59500    |\n",
            "| value_loss         | 0.379    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0562   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12000    |\n",
            "| policy_entropy     | 0.618    |\n",
            "| total_timesteps    | 60000    |\n",
            "| value_loss         | 0.00388  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=60500, episode_reward=2.45 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.154    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12100    |\n",
            "| policy_entropy     | 0.668    |\n",
            "| total_timesteps    | 60500    |\n",
            "| value_loss         | 0.0422   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=0.84 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.132    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12200    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 61000    |\n",
            "| value_loss         | 0.919    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=61500, episode_reward=1.88 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.132    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12300    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 61500    |\n",
            "| value_loss         | 0.00491  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=2.41 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.186    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12400    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 62000    |\n",
            "| value_loss         | 0.0269   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=62500, episode_reward=1.54 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.182   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12500    |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 62500    |\n",
            "| value_loss         | 0.00575  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=2.21 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.109    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12600    |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 63000    |\n",
            "| value_loss         | 0.0388   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=63500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -26.8    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12700    |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 63500    |\n",
            "| value_loss         | 0.148    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.556   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12800    |\n",
            "| policy_entropy     | 0.673    |\n",
            "| total_timesteps    | 64000    |\n",
            "| value_loss         | 0.00487  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64500, episode_reward=0.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.061    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 12900    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 64500    |\n",
            "| value_loss         | 0.00437  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.093   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13000    |\n",
            "| policy_entropy     | 0.638    |\n",
            "| total_timesteps    | 65000    |\n",
            "| value_loss         | 0.0164   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=65500, episode_reward=0.66 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.732    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13100    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 65500    |\n",
            "| value_loss         | 0.000137 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.144   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13200    |\n",
            "| policy_entropy     | 0.658    |\n",
            "| total_timesteps    | 66000    |\n",
            "| value_loss         | 0.0183   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=66500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.158    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13300    |\n",
            "| policy_entropy     | 0.681    |\n",
            "| total_timesteps    | 66500    |\n",
            "| value_loss         | 0.00288  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0116   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13400    |\n",
            "| policy_entropy     | 0.669    |\n",
            "| total_timesteps    | 67000    |\n",
            "| value_loss         | 0.000476 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=67500, episode_reward=0.85 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.401    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13500    |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 67500    |\n",
            "| value_loss         | 0.0946   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=0.85 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.509   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13600    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 68000    |\n",
            "| value_loss         | 0.00218  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=68500, episode_reward=2.34 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.328    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13700    |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 68500    |\n",
            "| value_loss         | 0.00118  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=2.31 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.874    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13800    |\n",
            "| policy_entropy     | 0.643    |\n",
            "| total_timesteps    | 69000    |\n",
            "| value_loss         | 0.0411   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=69500, episode_reward=2.29 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.144    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 13900    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 69500    |\n",
            "| value_loss         | 0.0319   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=2.31 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.241    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14000    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 70000    |\n",
            "| value_loss         | 0.00822  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70500, episode_reward=0.60 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0977   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14100    |\n",
            "| policy_entropy     | 0.684    |\n",
            "| total_timesteps    | 70500    |\n",
            "| value_loss         | 0.0759   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=0.67 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -24.6    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14200    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 71000    |\n",
            "| value_loss         | 0.00508  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=71500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.175   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14300    |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 71500    |\n",
            "| value_loss         | 0.0863   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.253   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14400    |\n",
            "| policy_entropy     | 0.669    |\n",
            "| total_timesteps    | 72000    |\n",
            "| value_loss         | 0.0686   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72500, episode_reward=2.81 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | 0.503    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14500    |\n",
            "| policy_entropy     | 0.687    |\n",
            "| total_timesteps    | 72500    |\n",
            "| value_loss         | 0.0128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=0.85 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -14.9    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14600    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 73000    |\n",
            "| value_loss         | 0.000302 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=73500, episode_reward=2.35 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2.08    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14700    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 73500    |\n",
            "| value_loss         | 0.318    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=0.67 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0345  |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14800    |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 74000    |\n",
            "| value_loss         | 0.0759   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=74500, episode_reward=0.67 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -8.26    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 14900    |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 74500    |\n",
            "| value_loss         | 0.00245  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=2.50 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.991   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15000    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 75000    |\n",
            "| value_loss         | 0.000144 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=75500, episode_reward=2.31 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.29    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15100    |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 75500    |\n",
            "| value_loss         | 0.0048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=2.41 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.32     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15200    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 76000    |\n",
            "| value_loss         | 0.00859  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=76500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.0283  |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15300    |\n",
            "| policy_entropy     | 0.683    |\n",
            "| total_timesteps    | 76500    |\n",
            "| value_loss         | 0.00216  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=2.94 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | 0.474    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15400    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 77000    |\n",
            "| value_loss         | 0.00699  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=77500, episode_reward=0.65 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.165    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15500    |\n",
            "| policy_entropy     | 0.691    |\n",
            "| total_timesteps    | 77500    |\n",
            "| value_loss         | 0.0159   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=0.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.144   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15600    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 78000    |\n",
            "| value_loss         | 0.0103   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=78500, episode_reward=0.64 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.26     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15700    |\n",
            "| policy_entropy     | 0.68     |\n",
            "| total_timesteps    | 78500    |\n",
            "| value_loss         | 0.052    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0599   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15800    |\n",
            "| policy_entropy     | 0.681    |\n",
            "| total_timesteps    | 79000    |\n",
            "| value_loss         | 0.00932  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=79500, episode_reward=2.59 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -3.07    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 15900    |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 79500    |\n",
            "| value_loss         | 0.000313 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -360     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16000    |\n",
            "| policy_entropy     | 0.612    |\n",
            "| total_timesteps    | 80000    |\n",
            "| value_loss         | 0.0644   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -4.58    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16100    |\n",
            "| policy_entropy     | 0.662    |\n",
            "| total_timesteps    | 80500    |\n",
            "| value_loss         | 0.000655 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=0.90 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -4.08    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16200    |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 81000    |\n",
            "| value_loss         | 0.000457 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=81500, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.699    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16300    |\n",
            "| policy_entropy     | 0.599    |\n",
            "| total_timesteps    | 81500    |\n",
            "| value_loss         | 0.138    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.839    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16400    |\n",
            "| policy_entropy     | 0.665    |\n",
            "| total_timesteps    | 82000    |\n",
            "| value_loss         | 0.000544 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=82500, episode_reward=2.72 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -25.3    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16500    |\n",
            "| policy_entropy     | 0.636    |\n",
            "| total_timesteps    | 82500    |\n",
            "| value_loss         | 0.00849  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=0.75 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.096    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16600    |\n",
            "| policy_entropy     | 0.651    |\n",
            "| total_timesteps    | 83000    |\n",
            "| value_loss         | 0.55     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=83500, episode_reward=1.53 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -2       |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16700    |\n",
            "| policy_entropy     | 0.674    |\n",
            "| total_timesteps    | 83500    |\n",
            "| value_loss         | 0.0135   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=2.31 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0539   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16800    |\n",
            "| policy_entropy     | 0.673    |\n",
            "| total_timesteps    | 84000    |\n",
            "| value_loss         | 0.0316   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84500, episode_reward=0.66 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.264    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 16900    |\n",
            "| policy_entropy     | 0.643    |\n",
            "| total_timesteps    | 84500    |\n",
            "| value_loss         | 0.775    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=2.52 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0975   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17000    |\n",
            "| policy_entropy     | 0.681    |\n",
            "| total_timesteps    | 85000    |\n",
            "| value_loss         | 0.00739  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85500, episode_reward=2.54 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.14    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17100    |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 85500    |\n",
            "| value_loss         | 0.00289  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=1.28 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.233   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17200    |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 86000    |\n",
            "| value_loss         | 0.0065   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86500, episode_reward=0.65 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -9.97    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17300    |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 86500    |\n",
            "| value_loss         | 0.00374  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=0.64 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.204    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17400    |\n",
            "| policy_entropy     | 0.68     |\n",
            "| total_timesteps    | 87000    |\n",
            "| value_loss         | 0.00987  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=87500, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.553   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17500    |\n",
            "| policy_entropy     | 0.615    |\n",
            "| total_timesteps    | 87500    |\n",
            "| value_loss         | 0.00533  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=0.70 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.228   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17600    |\n",
            "| policy_entropy     | 0.678    |\n",
            "| total_timesteps    | 88000    |\n",
            "| value_loss         | 0.00117  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88500, episode_reward=2.75 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.226   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17700    |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 88500    |\n",
            "| value_loss         | 0.0107   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3.03 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | 0.18     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17800    |\n",
            "| policy_entropy     | 0.684    |\n",
            "| total_timesteps    | 89000    |\n",
            "| value_loss         | 0.00136  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=89500, episode_reward=2.51 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -13.5    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 17900    |\n",
            "| policy_entropy     | 0.684    |\n",
            "| total_timesteps    | 89500    |\n",
            "| value_loss         | 0.000605 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=0.21 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -6.93    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18000    |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 90000    |\n",
            "| value_loss         | 0.0025   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90500, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -49.4    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18100    |\n",
            "| policy_entropy     | 0.601    |\n",
            "| total_timesteps    | 90500    |\n",
            "| value_loss         | 0.000157 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.491    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18200    |\n",
            "| policy_entropy     | 0.586    |\n",
            "| total_timesteps    | 91000    |\n",
            "| value_loss         | 0.0358   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91500, episode_reward=2.06 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -5.54    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18300    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 91500    |\n",
            "| value_loss         | 0.00281  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=0.49 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.232    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18400    |\n",
            "| policy_entropy     | 0.673    |\n",
            "| total_timesteps    | 92000    |\n",
            "| value_loss         | 0.00173  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92500, episode_reward=0.15 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.748    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18500    |\n",
            "| policy_entropy     | 0.646    |\n",
            "| total_timesteps    | 92500    |\n",
            "| value_loss         | 0.0205   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.019   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18600    |\n",
            "| policy_entropy     | 0.68     |\n",
            "| total_timesteps    | 93000    |\n",
            "| value_loss         | 0.00337  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93500, episode_reward=2.80 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.241    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18700    |\n",
            "| policy_entropy     | 0.668    |\n",
            "| total_timesteps    | 93500    |\n",
            "| value_loss         | 0.00521  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=2.73 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.0666   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18800    |\n",
            "| policy_entropy     | 0.659    |\n",
            "| total_timesteps    | 94000    |\n",
            "| value_loss         | 0.317    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94500, episode_reward=0.47 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -11.9    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 18900    |\n",
            "| policy_entropy     | 0.685    |\n",
            "| total_timesteps    | 94500    |\n",
            "| value_loss         | 0.00349  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3.14 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| explained_variance | -0.566   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19000    |\n",
            "| policy_entropy     | 0.68     |\n",
            "| total_timesteps    | 95000    |\n",
            "| value_loss         | 0.0545   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95500, episode_reward=0.07 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.16     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19100    |\n",
            "| policy_entropy     | 0.665    |\n",
            "| total_timesteps    | 95500    |\n",
            "| value_loss         | 0.42     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=2.54 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.648    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19200    |\n",
            "| policy_entropy     | 0.675    |\n",
            "| total_timesteps    | 96000    |\n",
            "| value_loss         | 0.0204   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96500, episode_reward=3.01 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | 0.613    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19300    |\n",
            "| policy_entropy     | 0.675    |\n",
            "| total_timesteps    | 96500    |\n",
            "| value_loss         | 0.00296  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=2.79 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -7.18    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 19400    |\n",
            "| policy_entropy     | 0.637    |\n",
            "| total_timesteps    | 97000    |\n",
            "| value_loss         | 0.00322  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97500, episode_reward=2.14 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -14      |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 19500    |\n",
            "| policy_entropy     | 0.676    |\n",
            "| total_timesteps    | 97500    |\n",
            "| value_loss         | 0.00257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=2.28 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -4.54    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 19600    |\n",
            "| policy_entropy     | 0.686    |\n",
            "| total_timesteps    | 98000    |\n",
            "| value_loss         | 0.00138  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98500, episode_reward=2.11 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -13.3    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 19700    |\n",
            "| policy_entropy     | 0.69     |\n",
            "| total_timesteps    | 98500    |\n",
            "| value_loss         | 0.000517 |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=2.28 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -0.5     |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19800    |\n",
            "| policy_entropy     | 0.689    |\n",
            "| total_timesteps    | 99000    |\n",
            "| value_loss         | 0.0285   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99500, episode_reward=2.72 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -1.11    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 19900    |\n",
            "| policy_entropy     | 0.634    |\n",
            "| total_timesteps    | 99500    |\n",
            "| value_loss         | 0.00761  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3.14 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| explained_variance | -3.37    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 20000    |\n",
            "| policy_entropy     | 0.672    |\n",
            "| total_timesteps    | 100000   |\n",
            "| value_loss         | 0.00442  |\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7f1ac64bfe90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHKxtD1us3fH",
        "outputId": "ac44d2f6-d15b-4274-dd8b-03ba5334f0fe"
      },
      "source": [
        "env = gym.make('stocks-v0', df=df, frame_bound=(90,100), window_size=5)\n",
        "obs = env.reset()\n",
        "while True: \n",
        "    obs = obs[np.newaxis, ...]\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    if done:\n",
        "        print(\"info\", info)\n",
        "        break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info {'total_reward': 0.7459999999999996, 'total_profit': 1.116579796102053, 'position': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}